doi,title,publication_date,subjects,abstract
10.1186/s13677-023-00550-3,Exploring cross-cultural and gender differences in facial expressions: a skin tone analysis using RGB Values,2023-11-20,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Facial expressions serve as crucial indicators of an individual's psychological state, playing a pivotal role in face-to-face communication. This research focuses on advancing collaboration between machines and humans by undertaking a thorough investigation into facial expressions. Specifically, we delve into the analysis of emotional variations related to changes in skin tone across different genders and cultural backgrounds (Black and white). The research methodology is structured across three phases. In Phase I, image data is acquired and meticulously processed from the Chicago face dataset, resulting in 12,402 augmented images across five classes (Normal case, Benign case, Adenocarcinoma, Squamous-cell-carcinoma, Large-cell-carcinoma). Phase II involves the identification of Regions of Interest (ROI) and the extraction of RGB values as features from these ROIs. Various methods, including those proposed by Kovac, Swift, and Saleh, are employed for precise skin identification. The final phase, Phase III, centers on the in-depth analysis of emotions and presents the research findings. Statistical techniques, such as Descriptive statistics, independent sample T-tests for gender and cross-cultural comparisons, and two-way ANOVA, are applied to RED, BLUE, and GREEN pixel values as response variables, with gender and emotions as explanatory variables. The rejection of null hypotheses prompts a Post Hoc test to discern significant pairs of means. The results indicate that both cross-cultural backgrounds and gender significantly influence pixel colors, underscoring the impact of different localities on pixel coloration. Across various expressions, our results exhibit a minimal 0.05% error rate in all classifications. Notably, the study reveals that green pixel color does not exhibit a significant difference between Anger and Neutral emotions, suggesting a near-identical appearance for green pixels in these emotional states. These findings contribute to a nuanced understanding of the intricate relationship between facial expressions, gender, and cultural backgrounds, providing valuable insights for future research in human–machine interaction and emotion recognition."
10.1186/s13677-023-00538-z,Minimize average tasks processing time in satellite mobile edge computing systems via a deep reinforcement learning method,2023-11-18,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Recently, the development of Low Earth Orbit (LEO) satellites and the advancement of the Mobile Edge Computing (MEC) paradigm have driven the emergence of the Satellite Mobile Edge Computing (Sat-MEC). Sat-MEC has been developed to support communication and task computation for Internet of Things (IoT) Mobile Devices (IMDs) in the absence of terrestrial networks. However, due to the heterogeneity of tasks and Sat-MEC servers, it is still a great challenge to efficiently schedule tasks in Sat-MEC servers. Here, we propose a scheduling algorithm based on the Deep Reinforcement Learning (DRL) method in the Sat-MEC architecture to minimize the average task processing time. We consider multiple factors, including the cooperation between LEO satellites, the concurrency and heterogeneity of tasks, the dynamics of LEO satellites, the heterogeneity of the computational capacity of Sat-MEC servers, and the heterogeneity of the initial queue for task computation. Further, we use the self-attention mechanism to act as a Q-network to extract high-dimensional dynamic information of tasks and Sat-MEC servers. In this work, we model the Sat-MEC environment simulation at the application level and propose a DRL-based task scheduling algorithm. The simulation results confirm the effectiveness of our proposed scheduling algorithm, which reduces the average task processing time by 22.1 $$\%$$ % , 30.6 $$\%$$ % , and 41.3 $$\%$$ % , compared to the genetic algorithm(GA), the greedy algorithm, and the random algorithm, respectively."
10.1186/s13677-023-00533-4,LAE-GAN: a novel cloud-based Low-light Attention Enhancement Generative Adversarial Network for unpaired text images,2023-11-18,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the widespread adoption of mobile multimedia devices, the deployment of compute-intensive inference tasks on edge and resource-constrained devices, particularly in the context of low-light text detection, remains a formidable challenge. Existing deep learning approaches have shown limited effectiveness in restoring images for extremely dark scenes. To address these limitations, this paper presents a novel cloud-based L ow-light A ttention E nhancement G enerative A dversarial N etwork for unpaired text images (LAE-GAN) for the non-paired text image enhancement task in extremely low-light conditions. In the first stage, compressed low-light images are transmitted from edge devices to a cloud server for image enhancement. The LAE-GAN, an end-to-end network comprising a Zero-DCE and AGM-net generator, is designed with a global and local discriminator structure. The initial illumination restoration of extremely low-light images is accomplished using the Zero-DCE network. To enhance text details, we propose an Enhanced Text Attention Mechanism (ETAM) that transforms text information into a comprehensive text attention mechanism across the entire network. The Sobel operator is employed to extract text edge information, while attention is focused on text region details through constraints imposed on the attention map and edge map. Additionally, an AGM-Net module is integrated to reduce noise and fine-tune illumination. In the second stage, the cloud server makes decisions based on user requirements and processes requests in parallel, scaling with the quantity of requests. In the third stage, the enhanced results are transmitted back to edge devices for text detection. Experimental results on widely used LOL and SID low-light datasets demonstrate significant improvements in both quantitative and qualitative analysis, surpassing state-of-the-art enhancement methods in terms of image restoration and text detection."
10.1186/s13677-023-00525-4,An internet of things enabled machine learning model for Energy Theft Prevention System (ETPS) in Smart Cities,2023-11-16,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Energy theft is a significant problem that needs to be addressed for effective energy management in smart cities. Smart meters are highly utilized in smart cities that help in monitoring the energy utilization level and provide information to the users. However, it is not able to detect energy theft or over-usage. Therefore, we have proposed a multi-objective diagnosing structure named an Energy Theft Prevention System (ETPS) to detect energy theft. The proposed system utilizes a combination of machine learning techniques Gated Recurrent Unit (GRU), Grey Wolf Optimization (GWO), Deep Recurrent Convolutional Neural Network (DDRCNN), and Long Short-Term Memory (LSTM). The statistical validation has been performed using the simple moving average (SMA) method. The results obtained from the simulation have been compared with the existing technique in terms of delivery ratio, throughput, delay, overhead, energy conversation, and network lifetime. The result shows that the proposed system is more effective than existing systems."
10.1186/s13677-023-00508-5,NuWa: off-state tolerant backscattering system with uncontrolled excitation traffics,2023-11-16,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Backscatter communication relies on passive reflections of the existing radio frequency (RF) signals, making it suitable for low-power and low-complexity communication in IoT applications. However, the performance of existing systems severely degrades in real-life environments, due to irregular “on” and “off” states of ambient signals like WiFi, which are not controllable. In this paper, we propose a joint coding and framing scheme for the backscattering physical layer to fight against the off-state in the excitation signal. We first design transmission schemes including both the Reed-Solomon (RS) codes and the frame structure, to correct the burst error caused by the off states. In order to implement the codes at the resource-constrained tag, we design a look-up table for the encoding process. We prototype our system NuWa that could efficiently backscatter with uncontrolled traffics generated randomly. We demonstrate that NuWa could achieve a 1 Mbps transmission throughput when the tag is over 1  m away from the receiver in high traffic load and 150 kbps in low traffic load. Finally, we evaluate the throughput with respect to the distance change between the tag and the receiver, and 950 kbps is achieved at a distance of $$6\,m$$ 6 m ."
10.1186/s13677-023-00540-5,Agent-based cloud simulation model for resource management,2023-11-15,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Driven by the successful service model and growing demand, cloud computing has evolved from a moderate-sized data center consisting of homogeneous resources to a heterogeneous hyper-scale computing ecosystem. This evolution has made the modern cloud environment increasingly complex. Large-scale empirical studies of essential concepts such as resource allocation, virtual machine migration, and operational cost reduction have typically been conducted using simulations. This paper presents an agent-based cloud simulation model for resource management. The focus is on how service placement strategies, service migration, and server consolidation affect the overall performance of homogeneous and heterogeneous clouds, in terms of energy consumption, resource utilization, and violation of service-level agreements. The main cloud elements are modeled as autonomous agents whose properties are encapsulated. The complex relationships between components are realized through asynchronous agent-to-agent interactions. Operating states and statistics are displayed in real time. In the evaluation, the efficiency of the simulator is studied empirically. The performance of various resource management algorithms is assessed using statistical methods, and the accuracy of server energy consumption models is examined. The results show that agent-based models can accurately reflect cloud status at a fine-grained level."
10.1186/s13677-023-00541-4,Research on electromagnetic vibration energy harvester for cloud-edge-end collaborative architecture in power grid,2023-11-13,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the deepening of the construction of the new type power system, the grid has become increasingly complex, and its safe and stable operation is facing more challenges. In order to improve the quality and efficiency of power grid management, State Grid Corporation continues to promote the digital transformation of the grid, proposing concepts such as cloud-edge-end collaborative architecture and power Internet of Things, for which comprehensive sensing of the grid is an important foundation. Power equipment is widely distributed and has a wide variety of types, and online monitoring of them involves the deployment and application of a large number of power sensors. However, there are various problems in implementing active power supplies for these sensors, which restrict their service life. In order to collect and utilize the vibration energy widely present in the grid to provide power for sensors, this paper proposes an electromagnetic vibration energy harvester and its design methodology based on a four-straight-beam structure, and carries out a trial production of prototype. The vibration pickup unit of the harvester is composed of polyimide cantilevers, a permanent magnet and a mass-adjusting spacer. The mass-adjusting spacer can control the vibration frequency of the vibration unit to match the target frequency. In this paper, a key novel method is proposed to increase the number of turns in a limited volume by stacking flexible coils, which can boost the output voltage of the energy harvester. A test system is built to conduct a performance test for the prototype harvester. According to the test results, the resonant frequency of the device is $$100\ Hz$$ 100 H z , the output peak-to-peak voltage at the resonant frequency is $$2.56\ V$$ 2.56 V at the acceleration of $$1\ g$$ 1 g , and the maximum output power is around $$151.7\ \mu W$$ 151.7 μ W . The proposed four-straight-beam electromagnetic vibration energy harvester in this paper has obvious advantages in output voltage and power compared with state-of-the-art harvesters. It can provide sufficient power for various sensors, support the construction of cloud-edge-end architecture and the deployment of a massive number of power sensors. In the last part of this article, a self-powered transformer vibration monitor is presented, demonstrating the practicality of the proposed vibration energy harvester."
10.1186/s13677-023-00535-2,FedEem: a fairness-based asynchronous federated learning mechanism,2023-11-09,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Federated learning is a mechanism for model training in distributed systems, aiming to protect data privacy while achieving collective intelligence. In traditional synchronous federated learning, all participants must update the model synchronously, which may result in a decrease in the overall model update frequency due to lagging participants. In order to solve this problem, asynchronous federated learning introduces an asynchronous aggregation mechanism, allowing participants to update models at their own time and rate, and then aggregate each updated edge model on the cloud, thus speeding up the training process. However, under the asynchronous aggregation mechanism, federated learning faces new challenges such as convergence difficulties and unfair model accuracy. This paper first proposes a fairness-based asynchronous federated learning mechanism, which reduces the adverse effects of device and data heterogeneity on the convergence process by using outdatedness and interference-aware weight aggregation, and promotes model personalization and fairness through an early exit mechanism. Mathematical analysis derives the upper bound of convergence speed and the necessary conditions for hyperparameters. Experimental results demonstrate the advantages of the proposed method compared to baseline algorithms, indicating the effectiveness of the proposed method in promoting convergence speed and fairness in federated learning."
10.1186/s13677-023-00515-6,Adaptive device sampling and deadline determination for cloud-based heterogeneous federated learning,2023-11-03,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","As a new approach to machine learning, Federated learning enables distributned traiing on edge devices and aggregates local models into a global model. The edge devices that participate in federated learning are highly heterogeneous in terms of computing power, device state, and data distribution, making it challenging to converge models efficiently. In this paper, we propose FedState, which is an adaptive device sampling and deadline determination technique for cloud-based heterogeneous federated learning. Specifically, we consider the cloud as a central server that orchestrates federated learning on a large pool of edge devices. To improve the efficiency of model convergence in heterogeneous federated learning, our approach adaptively samples devices to join each round of training and determines the deadline for result submission based on device state. We analyze existing device usage traces to build device state models in different scenarios and design a dynamic importance measurement mechanism based on device availability, data utility, and computing power. We also propose a deadline determination module that dynamically sets the deadline according to the availability of all sampled devices, local training time, and communication time, enabling more clients to submit local models more efficiently. Due to the variability of device state, we design an experience-driven algorithm based on Deep Reinforcement Learning (DRL) that can dynamically adjust our sampling and deadline policies according to the current environment state. We demonstrate the effectiveness of our approach through a series of experiments with the FMNIST dataset and show that our method outperforms current state-of-the-art approaches in terms of model accuracy and convergence speed."
10.1186/s13677-023-00531-6,Review on the application of cloud computing in the sports industry,2023-11-02,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The transformative impact of cloud computing has permeated various industries, reshaping traditional business models and accelerating digital transformations. In the sports industry, the adoption of cloud computing is burgeoning, significantly enhancing efficiency and unlocking new potentials. This paper provides a comprehensive review of the applications of cloud computing in the sports industry, focusing on areas such as athlete performance tracking, fan engagement, operations management, sports marketing, and event hosting. Moreover, the challenges and potential future developments of cloud computing applications in this industry are also discussed. The purpose of this review is to provide a thorough understanding of the state-of-the-art applications of cloud computing in the sports industry and to inspire further research and development in this field."
10.1186/s13677-023-00523-6,Improving cloud storage and privacy security for digital twin based medical records,2023-10-30,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","As digital transformation progresses across industries, digital twins have emerged as an important technology. In healthcare, digital twins are created by digitizing patient parameters, medical records, and treatment plans to enable personalized care, assist diagnosis, and improve planning. Data is core to digital twins, originating from physical and virtual entities as well as services. Once processed and integrated, data drives various components. Medical records are critical healthcare data but present unique challenges for digital twins. However, directly storing or encrypting medical records has issues. Plaintext risks privacy leaks while encryption hinders retrieval. To address this, we present a cloud-based solution combining post-quantum searchable encryption. Our system includes key generation using Physical Unable Functions (PUF). It encrypts medical records in cloud storage, verifies records using blockchain, and retrieves records via cloud. By integrating cloud encryption, blockchain verification and cloud retrieval, we propose a secure and efficient cloud-based medical records system for digital twins. Our implementation demonstrates the system provides users efficient and secure medical record services, compared to related designs. This highlights digital twins’ potential to transform healthcare through secure data-driven personalized care, diagnosis and planning."
10.1186/s13677-023-00521-8,sRetor: a semi-centralized regular topology routing scheme for data center networking,2023-10-25,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The performance of the data center network is critical for lowering costs and increasing efficiency. The software-defined networks (SDN) technique has been adopted in data center networks due to the recent emergence of advanced network control and flexibility demand. However, the rapid growth of data centers increases the complexity of control and management processes. With the rapid adoption of SDN, the following critical challenges arise in large-scale data center networks: 1) extra packet delay on the separated control plane and 2) controller bottleneck in large-scale topology. We propose sRetor in this paper, a topology-description-language-based routing approach for regular data center networks that leverages data center networks’ regularity. sRetor aims to reduce the packet waiting time and controller workload in software-defined data center networking. We propose to move partial forwarding decision-making from the controller to switches to eliminate unnecessary control plane delay and reduce controller workload. Therefore the sRetor controller is only responsible for troubleshooting complicated failures and on-demand traffic scheduling. Our numerical and experimental results show that sRetor reduces the flow start time by over 68% and the fail-over time by over 84%."
10.1186/s13677-023-00529-0,Intelligent acceptance systems for distribution automation terminals: an overview of edge computing technologies and applications,2023-10-23,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The investigation into intelligent acceptance systems for distribution automation terminals has spanned over a decade, furnishing indispensable assistance to the power industry. The integration of cutting-edge edge computing technologies into these systems has presented efficacious, low-latency, and energy-efficient remedies. This paper provides a comprehensive review and synthesis of research achievements in the field of intelligent acceptance systems for distribution automation terminals over the past few years. Firstly, this paper introduces the definition, composition, functions, and significance of distribution automation terminals, analyzes the advantages of employing edge computing in this domain, and elaborates on the design and implementation of intelligent acceptance systems based on edge computing technology. Additionally, this paper examines the technical challenges, security, and privacy issues associated with the application of edge computing in intelligent acceptance systems and proposes practical solutions. Finally, this paper summarizes the contributions and significance of this paper and provides an outlook on future research directions. It is evident from the review that the integration of edge computing has effectively alleviated these challenges, but new issues await resolution."
10.1186/s13677-023-00524-5,An edge server deployment method based on optimal benefit and genetic algorithm,2023-10-18,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the speedy advancement and accelerated popularization of 5G networks, the provision and request of services through mobile smart terminals have become a hot topic in the development of mobile service computing. In this scenario, an efficient and reasonable edge server deployment solution can effectively reduce the deployment cost and communication latency of mobile smart terminals, while significantly improving investment efficiency and resource utilization. Focusing on the issue of edge server placement in mobile service computing environment, this paper proposes an edge server deployment method based on optimal benefit quantity and genetic algorithm. This method is firstly, based on a channel selection strategy for optimal communication impact benefits, it calculates the quantity of edge servers which can achieve optimal benefit. Then, the issue of edge server deployment is converted to a dual-objective optimization problem under three constraints to find the best locations to deploy edge servers, according to balancing the workload of edge servers and minimizing the communication delay among clients and edge servers. Finally, the genetic algorithm is utilized to iteratively optimize for finding the optimal resolution of edge server deployment. A series of experiments are performed on the Mobile Communication Base Station Data Set of Shanghai Telecom, and the experimental results verify that beneath the limit of the optimal benefit quantity of edge servers, the proposed method outperforms MIP, K-means, ESPHA, Top-K, and Random in terms of effectively reducing communication delays and balancing workloads."
10.1186/s13677-023-00502-x,Reliability-aware failure recovery for cloud computing based automatic train supervision systems in urban rail transit using deep reinforcement learning,2023-10-17,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","As urban rail transit construction advances with information technology, modernization, information, and intelligence have become the direction of development. A growing number of cloud platforms are being developed for transit in urban areas. However, the increasing scale of urban rail cloud platforms, coupled with the deployment of urban rail safety applications on the cloud platform, present a huge challenge to cloud reliability.One of the key components of urban rail transit cloud platforms is Automatic Train Supervision (ATS). The failure of the ATS cloud service would result in less punctual trains and decreased traffic efficiency, making it essential to research fault tolerance methods based on cloud computing to improve the reliability of ATS cloud services. This paper proposes a proactive, reliability-aware failure recovery method for ATS cloud services based on reinforcement learning. We formulate the problem of penalty error decision and resource-efficient optimization using the advanced actor-critic (A2C) algorithm. To maintain the freshness of the information, we use Age of Information (AoI) to train the agent, and construct the agent using Long Short-Term Memory (LSTM) to improve its sensitivity to fault events. Simulation results demonstrate that our proposed approach, LSTM-A2C, can effectively identify and correct faults in ATS cloud services, improving service reliability."
10.1186/s13677-023-00527-2,A review of intelligent verification system distributiontautomationtterminalinal based on artificial intelligealgorithmsthms,2023-10-16,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Artificial intelligence (AI) plays a key role in the distribution automation system (DAS). By using artificial intelligence technology, it is possible to intelligently verify and monitor distribution automation terminals, improve their safety and reliability, and reduce power system operating and maintenance costs. At present, researchers are exploring a variety of application methods and algorithms of the distribution automation terminal intelligent acceptance system based on artificial intelligence, such as machine learning, deep learning and expert systems, and have made significant progress. This paper comprehensively reviews the existing research on the application of artificial intelligence technology in distribution automation systems, including fault detection, network reconfiguration, load forecasting, and network security. It undertakes a thorough examination and summarization of the major research achievements in the field of distribution automation systems over the past few years, while also analyzing the challenges that this field confronts. Moreover, this study elaborates extensively on the diverse applications of AI technology within distribution automation systems, providing a detailed comparative analysis of various algorithms and methodologies from multiple classification perspectives. The primary aim of this endeavor is to furnish valuable insights for researchers and practitioners in this domain, thereby fostering the advancement and innovation of distribution automation systems."
10.1186/s13677-023-00500-z,AI-enabled legacy data integration with privacy protection: a case study on regional cloud arbitration court,2023-10-14,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","This paper presents an interesting case study on Legacy Data Integration (LDI for short) for a Regional Cloud Arbitration Court. Due to the inconsistent structure and presentation, legacy arbitration cases can hardly integrate into the Cloud Court unless processed manually. In this study, we propose an AI-enabled LDI method to replace the costly manual approach and ensure privacy protection during the process. We trained AI models to replace tasks such as reading and understanding legacy cases, removing privacy information, composing new case records, and inputting them through the system interfaces. Our approach employs Optical Character Recognition (OCR), text classification, and Named Entity Recognition (NER) to transform legacy data into a system format. We applied our method to a Cloud Arbitration Court in Liaoning Province, China, and achieved a comparable privacy filtering effect while retaining the maximum amount of information. Our method demonstrated similar effectiveness as the manual LDI, but with greater efficiency, saving 90% of the workforce and achieving a 60%-70% information extraction rate compared to manual work. With the increasing development of informationalization and intelligentization in judgment and arbitration, many courts are adopting ABC technologies, namely Artificial intelligence, Big data, and Cloud computing, to build the court system. Our method provides a practical reference for integrating legal data into the system."
10.1186/s13677-023-00526-3,Correction to: Next-generation cyber attack prediction for IoT systems: leveraging multi-class SVM and optimized CHAID decision tree,2023-10-12,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems",
10.1186/s13677-023-00520-9,MapReduce scheduling algorithms in Hadoop: a systematic study,2023-10-10,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Hadoop is a framework for storing and processing huge volumes of data on clusters. It uses Hadoop Distributed File System (HDFS) for storing data and uses MapReduce to process that data. MapReduce is a parallel computing framework for processing large amounts of data on clusters. Scheduling is one of the most critical aspects of MapReduce. Scheduling in MapReduce is critical because it can have a significant impact on the performance and efficiency of the overall system. The goal of scheduling is to improve performance, minimize response times, and utilize resources efficiently. A systematic study of the existing scheduling algorithms is provided in this paper. Also, we provide a new classification of such schedulers and a review of each category. In addition, scheduling algorithms have been examined in terms of their main ideas, main objectives, advantages, and disadvantages."
10.1186/s13677-023-00497-5,Resource allocation strategy for blockchain-enabled NOMA-based MEC networks,2023-10-10,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Blockchain technology is getting more and more attention due to its decentralization, independence and security features. However, in wireless networks it faces a computational challenge: the proof-of-work problem. Mobile edge computing (MEC) leads to a vaild scheme by providing cloud computing capabilities to mobile devices. Non-orthogonal multiple access (NOMA) exploits the diversity properties in the power domain to further increase system throughput and spectral efficiency. In this paper, we suggest a new NOMA-based MEC wireless blockchain network to minimize system energy consumption through task offloading decision optimization, user clustering, computing resource and transmit power allocation. In order to effectively figure out this non-convex problem, we first propose a offloading decision and user clustering algorithm, and then propose a computing resource allocation algorithm based on user Quality of Service (QoS) requirements. Finally, the transmission power can be easily determined. The numerical simulation results verify that the proposed joint optimization algorithm can effectively decrease the system energy consumption."
10.1186/s13677-023-00518-3,Making programmable packet scheduling time-sensitive with a FIFO queue,2023-10-09,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Time-Sensitive Networking (TSN) is an emerging technology for real-time and non-real-time hybrid networked systems. TSN is standardized by IEEE 802.1 TSN Task Group and is becoming widely used in various scenarios including the cloud network. However, existing programmable packet schedulers such as PIFO, PIEO, and AIFO in programmable switches either lack the ability to express most scheduling algorithms in TSN or introduce intolerable on-chip memory overhead (e.g., strict-priority queues). This makes programmable switches and NICs incapable of providing deterministic forwarding. In this paper, we present AIAO (Admission-In-Admission-Out), a new set of programmable scheduling primitives using just a single FIFO to support typical TSN scheduling algorithms, as well as other popular work-conserving algorithms. AIAO is inspired by AIFO but improves it with a group of high-speed packet ingress/egress admission control triggered by high-precise and globally synchronized time, thus being able to support time-sensitive scheduling. We implement AIAO and evaluate it with FPGA-based TSN switches. The preliminary results show that AIAO guarantees correctness for a typical TSN scheduling algorithm with minimal logic and memory overhead."
10.1186/s13677-023-00522-7,Privacy and integrity-preserving data aggregation scheme for wireless sensor networks digital twins,2023-10-07,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The security technology of digital twin is an important guarantee to ensure the security of digital twin operation, which mainly includes network security technology, data security technology and privacy protection technology. In wireless sensor networks, data aggregation technologies are known as a suitable solution to reduce energy consumption. In addition, due to wireless communications, wireless sensor networks are subject to many attacks. Therefore, it is very important to provide data security in the data aggregation process. In this paper, in order to protect data privacy and verify data integrity, moreover, balance the energy consumption and security during the data aggregation, we present a privacy and integrity–preserving data aggregation scheme for wireless sensor networks based on digital twins technology and homomorphic fingerprinting (HFPIDA). The HFPIDA adopts privacy function to protect data privacy and adopts homomorphic fingerprinting technology to verify the aggregation data integrity. Security analysis shows that the HFPIDA can effectively preserve data privacy and verify data integrity. Simulation results show that the HFPIDA requires less communication and energy overheads, and can achieve higher aggregation accuracy."
10.1186/s13677-023-00519-2,A Hierarchical Optimized Resource Utilization based Content Placement (HORCP) model for cloud Content Delivery Networks (CDNs),2023-10-03,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Content Delivery Networks (CDNs) have grown in popularity as a result of the ongoing development of the Internet and its applications. The workload on streaming media service systems can be significantly decreased with the help of the cooperative edge-cloud computing architecture. In the traditional works, a different types of content placement and routing algorithms are developed for improving the content delivery of cloud systems with reduced delay and cost. But, the majority of existing algorithms facing complexities in terms of increased resource usage, ineffective delivery, and high system designing complexity. Therefore, the proposed work aims to develop a new framework, named as, Hierarchical Optimized Resource Utilization based Content Placement (HORCP) model for cloud CDNs. Here, the Chaotic Krill Herd Optimization (CKHO) method is used to optimize the resource usage for content placement. Then, a Hierarchical Probability Routing (HPR) model is employed to enable a dependable end-to-end data transmission with an optimized routing path. The performance of the proposed HORCP model is validated and compared by using several performance metrics. The obtained results are also compared with current state-of-the-art methodologies in order to show the superiority of the proposed HORCP model. By using the HORCP mechanism, the overall memory usage of the network is reduced to 80%, CPU usage is reduced to 20%, response is minimized to 2 s, and total congestion cost with respect to the network load level is reduced to 100."
10.1186/s13677-023-00514-7,Virtualized intelligent genetic load balancer for federated hybrid cloud environment using deep belief network classifier,2023-10-02,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Load balancing is major issue in federated cloud environment. Various services can be offered by different cloud service providers. As per current working environment cloud computing is used in major applications such as education, online shopping, multimedia services, etc. Dynamic load balancing is required to handle the resources. Federated cloud has various services offering system with computing resources, resource pooling, internet access services and storage. Intelligent Genetic algorithm is proposed to provide efficient load balancing service in hybrid cloud environment. Virtualized Intelligent Genetic Load Balancer algorithm consists of load balancer and resource provisioning system to allocate the resources. Enhanced Load Balancer is used to preserve the load and minimize the span time based on resource provisioning method. In this work we analyse automated virtual machine services by using runtime resource provision. Here we use enhanced load balancer to measure the performance using virtual machine placements, resource utilization and automated quality requirements. We design a deep belief network based on requirements and measure the accuracy using TensorFlow. The simulation results test the accuracy and compare the results. Virtualized Intelligent Genetic Load Balancer system is achieving the accuracy of 95% based on overall capacity requirements. We compare Virtualized Intelligent Genetic Load Balancer system performance with existing simulations results and compared the results."
10.1186/s13677-023-00517-4,Next-generation cyber attack prediction for IoT systems: leveraging multi-class SVM and optimized CHAID decision tree,2023-09-29,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Billions of gadgets are already online, making the IoT an essential aspect of daily life. However, the interconnected nature of IoT devices also leaves them open to cyber threats. The quantity and sophistication of cyber assaults aimed against Internet of Things (IoT) systems have skyrocketed in recent years. This paper proposes a next-generation cyber attack prediction framework for IoT systems. The framework uses the multi-class support vector machine (SVM) and the improved CHAID decision tree machine learning methods. IoT traffic is classified using a multi-class support vector machine to identify various types of attacks. The SVM model is then optimized with the help of the CHAID decision tree, which prioritizes the attributes most relevant to the categorization of attacks. The proposed framework was evaluated on a real-world dataset of IoT traffic. The findings demonstrate the framework's ability to categorize attacks accurately. The framework may determine which attributes are most crucial for attack categorization to enhance the SVM model's precision. The proposed technique focuses on network traffic characteristics that can be signs of cybersecurity threats on IoT networks and affected Network nodes. Selected feature vectors were also created utilizing the elements acquired on every IoT console. The evaluation results on the Multistep Cyber-Attack Dataset (MSCAD) show that the proposed CHAID decision tree can significantly predict the multi-stage cyber attack with 99.72% accuracy. Such accurate prediction is essential in managing cyber attacks in real-time communication. Because of its efficiency and scalability, the model may be used to forecast cyber attacks in real time, even in massive IoT installations. Because of its computing efficiency, it can make accurate predictions rapidly, allowing for prompt detection and action. By locating possible entry points for attacks and mitigating them, the framework helps strengthen the safety of IoT systems."
10.1186/s13677-023-00512-9,HAP-assisted multi-aerial base station deployment for capacity enhancement via federated deep reinforcement learning,2023-09-29,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Aerial base stations (AeBSs), as crucial components of air-ground integrated networks, are widely employed in cloud computing, disaster relief, and various applications. How to quickly and efficiently deploy multi-AeBSs for higher capacity gain has become a key research issue. In this paper, we address the 3D deployment optimization problem of multi-AeBSs with the objective of maximizing system capacity. To overcome communication overhead and privacy challenges in multi-agent deep reinforcement learning (MADRL), we propose a federated deep deterministic policy gradient (Fed-DDPG) algorithm for the multi-AeBS deployment decision. Specifically, a high-altitude platform (HAP)-assisted multi-AeBS deployment architecture is designed, in which low-altitude AeBS act as the local nodes to train its own deployment decision model, while the HAP acts as the global node to aggregate the weights of local models. In this architecture, AeBSs do not exchange raw data, addressing data privacy concerns and reducing communication overhead. Simulation results show that the proposed algorithm outperforms fully distributed MADRL algorithms and closely approximates the performance of multi-agent deep deterministic policy gradient (MADDPG), which requires global information during training, but with less training time."
10.1186/s13677-023-00516-5,"Orchestration in the Cloud-to-Things compute continuum: taxonomy, survey and future directions",2023-09-27,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","IoT systems are becoming an essential part of our environment. Smart cities, smart manufacturing, augmented reality, and self-driving cars are just some examples of the wide range of domains, where the applicability of such systems have been increasing rapidly. These IoT use cases often require simultaneous access to geographically distributed arrays of sensors, heterogeneous remote, local as well as multi-cloud computational resources. This gives birth to the extended Cloud-to-Things computing paradigm. The emergence of this new paradigm raised the quintessential need to extend the orchestration requirements (i.e., the automated deployment and run-time management) of applications from the centralised cloud-only environment to the entire spectrum of resources in the Cloud-to-Things continuum. In order to cope with this requirement, in the last few years, there has been a lot of attention to the development of orchestration systems in both industry and academic environments. This paper is an attempt to gather the research conducted in the orchestration for the Cloud-to-Things continuum landscape and to propose a detailed taxonomy, which is then used to critically review the landscape of existing research work. We finally discuss the key challenges that require further attention and also present a conceptual framework based on the conducted analysis."
10.1186/s13677-023-00509-4,Intelligent intrusion detection framework for multi-clouds – IoT environment using swarm-based deep learning classifier,2023-09-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","In the current era, a tremendous volume of data has been generated by using web technologies. The association between different devices and services have also been explored to wisely and widely use recent technologies. Due to the restriction in the available resources, the chance of security violation is increasing highly on the constrained devices. IoT backend with the multi-cloud infrastructure to extend the public services in terms of better scalability and reliability. Several users might access the multi-cloud resources that lead to data threats while handling user requests for IoT services. It poses a new challenge in proposing new functional elements and security schemes. This paper introduces an intelligent Intrusion Detection Framework (IDF) to detect network and application-based attacks. The proposed framework has three phases: data pre-processing, feature selection and classification. Initially, the collected datasets are pre-processed using Integer- Grading Normalization (I-GN) technique that ensures a fair-scaled data transformation process. Secondly, Opposition-based Learning- Rat Inspired Optimizer (OBL-RIO) is designed for the feature selection phase. The progressive nature of rats chooses the significant features. The fittest value ensures the stability of the features from OBL-RIO. Finally, a 2D-Array-based Convolutional Neural Network (2D-ACNN) is proposed as the binary class classifier. The input features are preserved in a 2D-array model to perform on the complex layers. It detects normal (or) abnormal traffic. The proposed framework is trained and tested on the Netflow-based datasets. The proposed framework yields 95.20% accuracy, 2.5% false positive rate and 97.24% detection rate."
10.1186/s13677-023-00511-w,Simcan2Cloud: a discrete-event-based simulator for modelling and simulating cloud computing infrastructures,2023-09-18,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud computing is an evolving paradigm whose adoption has been increasing over the last few years. This fact has led to the growth of the cloud computing market, together with fierce competition for the leading market share, with an increase in the number of cloud service providers. Novel techniques are continuously being proposed to increase the cloud service provider’s profitability. However, only those techniques that are proven not to hinder the service agreements are considered for production clouds. Analysing the expected behaviour and performance of the cloud infrastructure is challenging, as the repeatability and reproducibility of experiments on these systems are made difficult by the large number of users concurrently accessing the infrastructure. To this, must be added the complications of using different provisioning policies, managing several workloads, and applying different resource configurations. Therefore, in order to alleviate these issues, we present Simcan2Cloud, a discrete-event-based simulator for modelling and simulating cloud computing environments. Simcan2Cloud focuses on modelling and simulating the behaviour of the cloud provider with a high level of detail, where both the cloud infrastructure and the interactions of the users with the cloud are integrated in the simulated scenarios. For this purpose, Simcan2Cloud supports different resource allocation policies, service level agreements (SLAs), and an intuitive and complete API for including new management policies. Finally, a thorough experimental study to measure the suitability and applicability of Simcan2Cloud, using both real-world traces and synthetic workloads, is presented."
10.1186/s13677-023-00506-7,Stateless Q-learning algorithm for service caching in resource constrained edge environment,2023-09-13,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","In resource constrained edge environment, multiple service providers can compete to rent the limited resources to cache their service instances on edge servers close to end users, thereby significantly reducing the service delay and improving quality of service (QoS). However, service providers renting the resources of different edge servers to deploy their service instances can incur different resource usage costs and service delay. To make full use of the limited resources of the edge servers to further reduce resource usage costs, multiple service providers on an edge server can form a coalition and share the limited resource of an edge server. In this paper, we investigate the service caching problem of multiple service providers in resource constrained edge environment, and propose an independent learners-based services caching scheme (ILSCS) which adopts a stateless Q-learning to learn an optimal service caching scheme. To verify the effectiveness of ILSCS scheme, we implement COALITION, RANDOM, MDU, and MCS four baseline algorithms, and compare the total collaboration cost and service latency of ILSCS scheme with these of these four baseline algorithms under different experimental parameter settings. The extensive experimental results show that the ILSCS scheme can achieve lower total collaboration cost and service latency."
10.1186/s13677-023-00503-w,A deep reinforcement learning assisted task offloading and resource allocation approach towards self-driving object detection,2023-09-12,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the development of communication technology and mobile edge computing (MEC), self-driving has received more and more research interests. However, most object detection tasks for self-driving vehicles are still performed at vehicle terminals, which often requires a trade-off between detection accuracy and speed. To achieve efficient object detection without sacrificing accuracy, we propose an end–edge collaboration object detection approach based on Deep Reinforcement Learning (DRL) with a task prioritization mechanism. We use a time utility function to measure the efficiency of object detection task and aim to provide an online approach to maximize the average sum of the time utilities in all slots. Since this is an NP-hard mixed-integer nonlinear programming (MINLP) problem, we propose an online approach for task offloading and resource allocation based on Deep Reinforcement learning and Piecewise Linearization (DRPL). A deep neural network (DNN) is implemented as a flexible solution for learning offloading strategies based on road traffic conditions and wireless network environment, which can significantly reduce computational complexity. In addition, to accelerate DRPL network convergence, DNN outputs are grouped by in-vehicle cameras to form offloading strategies via permutation. Numerical results show that the DRPL scheme is at least 10% more effective and superior in terms of time utility compared to several representative offloading schemes for various vehicle local computing resource scenarios."
10.1186/s13677-023-00507-6,Dynamic deployment method based on double deep Q-network in UAV-assisted MEC systems,2023-09-01,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The unmanned aerial vehicle (UAV) assisted mobile edge computing (MEC) system leverages the high maneuverability of UAVs to provide efficient computing services to terminals. A dynamic deployment algorithm based on double deep Q-networks (DDQN) is suggested to address issues with energy limitation and obstacle avoidance when providing edge services to terminals by UAV. First, the energy consumption of the UAV and the fairness of the terminal’s geographic location are jointly optimized in the case of multiple obstacles and multiple terminals on the ground. And the UAV can avoid obstacles. Furthermore, a double deep Q-network was introduced to address the slow convergence and risk of falling into local optima during the optimization problem training process. Also included in the learning process was a pseudo count exploration strategy. Finally, the improved DDQN algorithm achieves faster convergence and a higher average system reward, according to experimental results. Regarding the fairness of geographic locations of terminals, the improved DDQN algorithm outperforms Q-learning, DQN, and DDQN algorithms by 50%, 20%, and 15.38%, respectively, and the stability of the improved algorithm is also validated."
10.1186/s13677-023-00499-3,AMAKAS: Anonymous Mutual Authentication and Key Agreement Scheme for securing multi-server environments,2023-08-30,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The rapid growth of Internet users was the motivation of the emerge appearance of new computing models such as cloud computing, fog computing and edge computing. For this reason, the multi-server’s architecture has been introduced to extend scalability and accessibility. To ensure that these servers can only be accessed by the authorized users, many authentication and key agreement schemes have been introduced for multi–server environments. In this paper, we propose an anonymous mutual authentication and key agreement scheme for multi-server architecture based on elliptic curve cryptography to achieve the required security services and resist the well-known security attacks. Furthermore, formal and informal security analysis is conducted to prove the security of the proposed scheme. Moreover, we provide a performance comparison with related work in terms of computational cost, communication cost and the number of messages transferred on the public channel. This performance comparison clearly shows that the proposed scheme is highly efficient in terms of computation, communication cost and security analysis as compared to other related schemes which makes the proposed scheme more suitable and practical for multi-server environments than other related schemes."
10.1186/s13677-023-00505-8,AI-empowered game architecture and application for resource provision and scheduling in multi-clouds,2023-08-30,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Current deep learning technologies used a large number of parameters to achieve a high accuracy rate, and the number of parameters is commonly more than a hundred million for image-related tasks. To improve both training speed and accuracy in multi-clouds, distributed deep learning is also widely applied. Therefore, reducing the network scale or improving the training speed has become an urgent problem to be solved in multi-clouds. Concerning this issue, we proposed a game architecture in multi-clouds, which can be supported by resource provision and service schedule. Furthermore, we trained a deep learning network, which can ensure high accuracy while reducing the number of network parameters. An adapted game, called flappy bird, is used as an experimental environment to test our neural network. Experimental results showed that the decision logic of the flappy bird, including flight planning, avoidance, and sacrifice, is accurate. In addition, we published the parameters of the neural network, so other scholars can reuse our neural network parameters for further research."
10.1186/s13677-023-00491-x,Intrusion detection in cloud computing based on time series anomalies utilizing machine learning,2023-08-29,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The growth of cloud computing is hindered by concerns about privacy and security. Despite the widespread use of network intrusion detection systems (NIDS), the issue of false positives remains prevalent. Furthermore, few studies have approached the intrusion detection problem as a time series issue, requiring time series modeling. In this study, we propose a novel technique for the early detection of intrusions in cloud computing using time series data. Our approach involves a method for Feature Selection (FS) and a prediction model based on the Facebook Prophet model to assess its efficiency. The FS method we propose is a collaborative feature selection model that integrates time series analysis techniques with anomaly detection, stationary, and causality tests. This approach specifically addresses the challenge of misleading connections between time series anomalies and attacks. Our results demonstrate a significant reduction in predictors employed in our prediction model, from 70 to 10 predictors, while improving performance metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), Median Absolute Percentage Error (MdAPE), and Dynamic Time Warping (DTW). Furthermore, our approach has resulted in reduced training, prediction, and cross-validation times of approximately 85%, 15%, and 97%, respectively. Although memory consumption remains similar, the utilization time has been significantly reduced, resulting in substantial resource usage reduction. Overall, our study presents a comprehensive methodology for effective early detection of intrusions in cloud computing based on time series anomalies, employing a collaborative feature selection model and the Facebook Prophet prediction model. Our findings highlight the efficiency and performance improvements achieved through our approach, contributing to the advancement of intrusion detection techniques in the context of cloud computing security."
10.1186/s13677-023-00492-w,MFGAD-INT: in-band network telemetry data-driven anomaly detection using multi-feature fusion graph deep learning,2023-08-28,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","As the cloud services market grows, cloud management tools that detect network anomalies in a non-intrusive manner are critical to improve users’ experience of cloud services. However, some network anomalies, such as Microburst, in cloud systems are very discreet. Network monitoring methods, e.g., SNMP, Ping, are of coarse temporal granularity or low-dimension metrics, have difficulty to identify such anomalies quickly and accurately. Network telemetry is able to collect rich network metrics with fine temporal granularity, which can provide deep insight into network anomalies. However, the rich features in the telemetry data are insufficient exploited in existing research. This paper proposes a Multi-feature Fusion Graph Deep learning approach driven by the In-band Network Telemetry, shorted as MFGAD-INT, to efficiently extract and process the spatial-temporal correlation information in telemetry data and effectively identify the anomalies. The experimental results show that the accuracy performance of the proposed method improves about 10.56% compared to the anomaly detection method without network telemetry and about 9.73% compared to the network telemetry-based method."
10.1186/s13677-023-00501-y,Energy-efficient virtual machine placement in distributed cloud using NSGA-III algorithm,2023-08-26,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud computing is the most widely adapted computing model to process scientific workloads in remote servers accessed through the internet. In the IaaS cloud, the virtual machine (VM) is the execution unit that processes the user workloads. Virtualization enables the execution of multiple virtual machines (VMs) on a single physical machine (PM). Virtual machine placement (VMP) strategically assigns VMs to suitable physical devices within a data center. From the cloud provider's perspective, the virtual machine must be placed optimally to reduce resource wastage to aid economic revenue and develop green data centres. Cloud providers need an efficient methodology to minimize resource wastage, power consumption, and network transmission delay. This paper uses NSGA-III, a multi-objective evolutionary algorithm, to simultaneously reduce the mentioned objectives to obtain a non-dominated solution. The performance metrics (Overall Nondominated Vector Generation and Spacing) of the proposed NSGA-III algorithm is compared with other multi-objective algorithms, namely VEGA, MOGA, SPEA, and NSGA-II. It is observed that the proposed algorithm performs 7% better that the existing algorithm in terms of ONVG and 12% better results in terms of spacing. ANOVA and DMRT statistical tests are used to cross-validate the results."
10.1186/s13677-023-00504-9,Data-intensive workflow scheduling strategy based on deep reinforcement learning in multi-clouds,2023-08-26,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the increase development of Internet of Things devices, the data-intensive workflow has emerged as a new kinds of representation for IoT applications. Because most IoT systems are structured in multi-clouds environment and the data-intensive workflow has the characteristics of scattered data sources and distributed execution requirements at the cloud center and edge clouds, it brings many challenges to the scheduling of such workflow, such as data flow control management, data transmission scheduling, etc. Aiming at the execution constraints of business and technology and data transmission optimization of data-intensive workflow, a data-intensive workflow scheduling method based on deep reinforcement learning in multi-clouds is proposed. First, the execution constraints, edge node load and data transmission volume of IoT data workflow are modeled; then the data-intensive workflow is segmented with the consideration of business constraints and the first optimization goal of data transmission; besides, taking the workflow execution time and average load balancing as the secondary optimization goal, the improved DQN algorithm is used to schedule the workflow. Based on the DQN algorithm, the model reward function and action selection are redesigned and improved. The simulation results based on WorkflowSim show that, compared with MOPSO, NSGA-II, GTBGA and DQN, the algorithm proposed in this paper can effectively reduce the execution time of IoT data workflow under the condition of ensuring the execution constraints and load balancing of multi-clouds."
10.1186/s13677-023-00510-x,COCAM: a cooperative video edge caching and multicasting approach based on multi-agent deep reinforcement learning in multi-clouds environment,2023-08-24,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","['The evolution of the Internet of Things technology (IoT) has boosted the drastic increase in network traffic demand. Caching and multicasting in the multi-clouds scenario are effective approaches to alleviate the backhaul burden of networks and reduce service latency. However, existing works do not jointly exploit the advantages of these two approaches. In this paper, we propose COCAM, a cooperative video edge caching and multicasting approach based on multi-agent deep reinforcement learning to minimize the transmission number in the multi-clouds scenario with limited storage capacity in each edge cloud. Specifically, by integrating a cooperative transmission model with the caching model, we provide a concrete formulation of the joint problem. Then, we cast this decision-making problem as a multi-agent extension of the Markov decision process and propose a multi-agent actor-critic algorithm in which each agent learns a local caching strategy and further encompasses the observations of neighboring agents as constituents of the overall state. Finally, to validate the COCAM algorithm, we conduct extensive experiments on a real-world dataset. The results show that our proposed algorithm outperforms other baseline algorithms in terms of the number of video transmissions.', 'Graphical Abstract', '']"
10.1186/s13677-023-00494-8,Blockchain based trusted execution environment architecture analysis for multi - source data fusion scenario,2023-08-19,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Multi-source data fusion techniques are widely applied in dynamic target detection scenarios, such as target situational awareness, radar signal resolution, and feature fusion labeling. Currently, techniques including clustering, neural networks, Bayesian analysis, and machine learning have been applied to improve the success rate of multi-source data fusion in terms of interference data noise reduction. The research on data tampering prevention of multiple data sources is mainly based on the data distributed authentication technology. The research on performing data fusion process in a trusted execution environment is mainly based on cryptography and codec technology. This paper focuses on the technical application architecture that can effectively improve the comprehensive efficiency of multi-source data fusion processing under the constraints of business scenarios. Accordingly, this paper proposes a trusted execution environment architecture based on blockchain technology for multi-source data fusion scenarios. It integrates the strategy of trusted data source data verification in blockchain smart contracts into the typical multi-source data fusion application architecture. After comparison tests in a simulation environment, the trusted execution environment architecture based on blockchain technology has shown considerable improvements in fusion success rate with limited performance cost."
10.1186/s13677-023-00498-4,Joint optimization of energy trading and consensus mechanism in blockchain-empowered smart grids: a reinforcement learning approach,2023-08-19,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Under the trend of green development, the traditional fossil fuel and centralized energy management models are no longer applicable, and distributed energy systems that can efficiently utilize clean energy have become the key to research in the energy field nowadays. However, there are still many problems in distributed energy trading systems, such as user privacy protection and mutual trust in trading, how to ensure the high quality and reliability of energy services, and how to motivate energy suppliers to participate in trading. To solve these problems, this paper proposes a blockchain-based smart grid system that enables efficient energy trading and consensus optimization, enabling electricity consumers to obtain high-quality, reliable energy services and electricity suppliers to receive rich rewards, and motivating all parties to actively participate in trading to maintain the balance of the system. We propose a reputation value assessment algorithm to evaluate the reputation of electricity suppliers to ensure that electricity consumers receive quality energy services. To minimize the cost, maximize the benefit for the electricity suppliers and optimize the system, we present an algorithm based on reinforcement learning DDPG to determine the power supplier, power generation capacity, and consensus mechanism between nodes to obtain power trading rights in each round. Simulation results show that the proposed energy trading scheme has good performance in terms of rewards."
10.1186/s13677-023-00496-6,Blockchain enabled task offloading based on edge cooperation in the digital twin vehicular edge network,2023-08-11,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The rapid development of the Internet of Vehicles (IoV) along with the emergence of intelligent applications have put forward higher requirements for massive task offloading. Even though Mobile Edge Computing (MEC) can diminish network transmission delay and ease network congestion, the constrained heterogeneous resources of a single edge server and the highly dynamic topology of vehicular edge networks may compromise the efficiency of task offloading, including latency and energy consumption. Vehicular edge networks are also vulnerable to malicious outside attacks. In this paper, we propose a new blockchain-enabled digital twin vehicular edge network (DTVEN) where digital twin (DT) is exploited to monitor network communication, computation, and caching (3C) resources management in real time to provide rich data for offloading decision-making, and blockchain is utilized to secure fair and decentralized offloading transactions among DTs. To ensure 3C resources sharing across edge servers, we design a DT-assisted edge cooperation scheme, which makes full use of edge resources in vehicular networks. Furthermore, a DT-based smart contract is built to achieve a quick and effective consensus process. Then, we apply a task offloading algorithm based on an improved cuckoo algorithm (ICA) and a resource allocation scheme based on greedy strategy to minimize network cost by comprehensively taking into account latency and energy consumption. Numerical results demonstrate that our proposed scheme outperforms the existing schemes in terms of network cost."
10.1186/s13677-023-00483-x,An LSTM based cross-site scripting attack detection scheme for Cloud Computing environments,2023-08-08,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud Computing plays a pivotal role in facilitating the Internet of Things (IoT) and its diverse applications. Users frequently access and store data on remote servers in Cloud Computing environments through web browsers. Consequently, attackers may exploit vulnerabilities in web browsing to embed malicious code into web pages, enabling them to launch attacks on remote servers in Cloud Computing environments. Due to its complexity, prevalence, and significant impact, XSS has consistently been recognized as one of the top ten web security vulnerabilities by OWASP. The existing XSS detection technology requires optimization: manual feature extraction is time-consuming and heavily reliant on domain knowledge, while the current confusion technology and complex code logic contribute to a decline in the identification of XSS attacks. This paper proposes a character-level bidirectional long-term and short-term memory network model based on a multi-attention mechanism. The bidirectional long-term and short-term memory network ensures the association of current features with preceding and subsequent text, while the multi-attention mechanism extracts additional features from different feature subspaces to enhance the understanding of text semantics. Experimental results demonstrate the effectiveness of the proposed model for XSS detection, with an F1 score of 98.71%."
10.1186/s13677-023-00488-6,Collaborative on-demand dynamic deployment via deep reinforcement learning for IoV service in multi edge clouds,2023-08-08,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","In vehicular edge computing, the low-delay services are invoked by the vehicles from the edge clouds while the vehicles moving on the roads. Because of the insufficiency of computing capacity and storage resource for edge clouds, a single edge cloud cannot handle all the services, and thus the efficient service deployment strategy in multi edge clouds should be designed according to the service demands. Noticed that the service demands are dynamic in temporal, and the inter-relationship between services is a non-negligible factor for service deployment. In order to address the new challenges produced by these factors, a collaborative service on-demand dynamic deployment approach with deep reinforcement learning is proposed, which is named CODD-DQN. In our approach, the number of service request of each edge clouds are forecasted by a time-aware service demands prediction algorithm, and then the interacting services are discovered through the analysis of service invoking logs. On this basis, the service response time models are constructed to formulated the problem, aiming to minimize service response time with data transmission delay between services. Furthermore, a collaborative service dynamic deployment algorithm with DQN model is proposed to deploy the interacting services. Finally, the real-world dataset based experiments are conducted. The results show our approach can achieve lowest service response time than other algorithms for service deployment."
10.1186/s13677-023-00495-7,ROMSS: a rational optional multi-secret sharing scheme based on reputation mechanism,2023-08-05,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The traditional threshold secret sharing scheme only allows the participants’ sub-secret shares to be used once in the reconstruction process. Several multi-secret sharing schemes have been proposed that are related to cloud computing, aiming to improve reconstruction efficiency. Rational secret sharing is a technique that combines secret sharing with game theory. In traditional rational multi-secret sharing, participants must reconstruct all secrets, resulting in unnecessary overhead. Rational participants will act dishonestly to maximize their own interests, leading to a prisoner’s dilemma and incomplete secret reconstruction. Additionally, when sharing multiple secrets, the Dealer must distribute the sub-secret shares of all secrets to the participants, increasing overhead. In this paper, we propose a rational optional multi-secret sharing scheme based on a reputation mechanism that selectively reconstructs secrets according to participants’ needs in the context of cloud computing. Our scheme introduces a reputation mechanism to evaluate participants’ reputation values to avoid their dishonest behaviors. Furthermore, we adopt a broadcast encryption matrix so that participants only need to receive a single sub-secret share to participate in multi-secret reconstruction. Our security analysis shows that the proposed scheme can effectively constrain the self-interested behavior of rational participants and reduce the overhead in the process, thus multi-secret sharing scheme can provide more efficient and secure solutions for secret sharing in key management and distributive storage for the cloud scenarios."
10.1186/s13677-023-00493-9,Joint DNN partitioning and task offloading in mobile edge computing via deep reinforcement learning,2023-08-03,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","As Artificial Intelligence (AI) becomes increasingly prevalent, Deep Neural Networks (DNNs) have become a crucial tool for developing and advancing AI applications. Considering limited computing and energy resources on mobile devices (MDs), it is a challenge to perform compute-intensive DNN tasks on MDs. To attack this challenge, mobile edge computing (MEC) provides a viable solution through DNN partitioning and task offloading. However, as the communication conditions between different devices change over time, DNN partitioning on different devices must also change synchronously. This is a dynamic process, which aggravates the complexity of DNN partitioning. In this paper, we delve into the issue of jointly optimizing energy and delay for DNN partitioning and task offloading in a dynamic MEC scenario where each MD and the server adopt the pre-trained DNNs for task inference. Taking advantage of the characteristics of DNN, we first propose a strategy for layered partitioning of DNN tasks to divide the task of each MD into subtasks that can be either processed on the MD or offloaded to the server for computation. Then, we formulate the trade-off between energy and delay as a joint optimization problem, which is further represented as a Markov decision process (MDP). To solve this, we design a DNN partitioning and task offloading (DPTO) algorithm utilizing deep reinforcement learning (DRL), which enables MDs to make optimal offloading decisions. Finally, experimental results demonstrate that our algorithm outperforms existing non-DRL and DRL algorithms with respect to processing delay and energy consumption, and can be applied to different DNN types."
10.1186/s13677-023-00489-5,A method to recommend cloud manufacturing service based on the spectral clustering and improved Slope one algorithm,2023-08-03,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The booming growth of cloud manufacturing services provides users with more choices. However, cloud manufacturing service recommendation remains a challenging issue due to numerous similar candidate services and diverse user preferences. The purpose of this paper is to provide an efficient and accurate cloud manufacturing service recommendation method. A spectral clustering algorithm is first designed to cluster the cloud manufacturing services. Then the candidate rating service set is constructed based on the service clusters by service function comparison and parameter matching. Finally, an improved Slope one algorithm, which integrates user similarity and service similarity, is proposed to rate the cloud manufacturing services. The top- k services with the highest scores are recommended to the users. Experiments show that the proposed method can provide more accurate service rating with less time consumption. The service recommendation performance of this method is also proved to be superior to other methods in terms of precision, recall, and F-score."
10.1186/s13677-023-00486-8,An efficient and scalable vaccine passport verification system based on ciphertext policy attribute-based encryption and blockchain,2023-08-02,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Implementing a trust and secure immunity or vaccine passport verification system is now crucial for many countries. The system typically aims to enable the secure access control and verification of vaccination records which will be used by trusted parties. However, the issues related to the system scalability in supporting a large number of data access requests, the enforcement of the user consent for data sharing, and the flexibility in delegating the access capability to trusted parties have not been resolved by existing works. In this paper, we propose a Universal Vaccine Passport Verification System (UniVAC) to support a decentralized, scalable, secure, and fine-grained, access control for Covid-19 vaccine passport data sharing and verification. At a core of our scheme, we employ the ciphertext policy attribute-based encryption (CP-ABE) to support secure and fine-grained access control and use the blockchain to record access transactions and provide data indexing. Furthermore, we propose a ciphertext retrieval method based on regional blockchain segmentation and introduce the outsourced CP-ABE decryption as a part of the proxy re-encryption (PRE) process to enable scalable and secure ciphertext delivery of the encrypted vaccine passport under the requestor’s public key. Finally, we conducted the extensive experiments in real cloud environment and the results showed that our proposed scheme is more efficient and scalable than related works."
10.1186/s13677-023-00490-y,UDL: a cloud task scheduling framework based on multiple deep neural networks,2023-07-28,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud task scheduling and resource allocation (TSRA) constitute a core issue in cloud computing. Batch submission is a common user task deployment mode in cloud computing systems. In this mode, it has been a challenge for cloud systems to balance the quality of user service and the revenue of cloud service provider (CSP). To this end, with multi-objective optimization (MOO) of minimizing task latency and energy consumption, we propose a cloud TSRA framework based on deep learning (DL). The system solves the TSRA problems of multiple task queues and virtual machine (VM) clusters by uniting multiple deep neural networks (DNNs) as task scheduler of cloud system. The DNNs are divided into exploration part and exploitation part. At each scheduling time step, the model saves the best outputs of all scheduling policies from each DNN to the experienced sample memory pool (SMP), and periodically selects random training samples from SMP to train each DNN of exploitation part. We designed a united deep learning (UDL) algorithm based on this framework. Experimental results show that the UDL algorithm can effectively solve the MOO problem of TSRA for cloud tasks, and performs better than benchmark algorithms such as heterogeneous distributed deep learning (HDDL) in terms of task scheduling performance."
10.1186/s13677-023-00487-7,The product quality inspection scheme based on software-defined edge intelligent controller in industrial internet of things,2023-07-27,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The Industrial Internet of Things (IIoT) enables the improvement of the productivity and intelligent level of factory. The procedure of product quality inspection has generally adopted machine intelligence algorithms instead of manual operation to improve efficiency. In this paper, we propose a product quality inspection system scheme based on software-defined edge intelligent controller (SD-EIC). By adopting the software definition and resource virtualization technologies, the hardware platform of SD-EIC is designed to support the real-time control tasks and non-real-time edge computing tasks at the same time. To this end, we propose the scheme and architecture of product quality inspection system based on SD-EIC. Multiple virtual controllers and virtual edge computing nodes are constructed on a set of SD-EIC hardware platform to realize the integrated deployment of the real-time control for terminal devices and the AI model reasoning of product defect recognition algorithm based on machine vision respectively. In addition, the management and control scheme of product quality inspection system based on industrial information model is proposed. By constructing the semantic-based digital twin information model of terminal device, the flexible adjustment and parameter configuration of terminal device are realized to meet the demands of flexible production and manufacturing. The proposed product quality inspection system solution can effectively improve the utilization of hardware resources and the efficiency of product quality inspection, and reduce the overall deployment cost of the system. It can flexibly adapt to product diversity and different industrial scenarios."
10.1186/s13677-023-00461-3,Optimizing task offloading and resource allocation in edge-cloud networks: a DRL approach,2023-07-26,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Edge-cloud computing is an emerging approach in which tasks are offloaded from mobile devices to edge or cloud servers. However, Task offloading may result in increased energy consumption and delays, and the decision to offload the task is dependent on various factors such as time-varying radio channels, available computation resources, and the location of devices. As edge-cloud computing is a dynamic and resource-constrained environment, making optimal offloading decisions is a challenging task. This paper aims to optimize offloading and resource allocation to minimize delay and meet computation and communication needs in edge-cloud computing. The problem of optimizing task offloading in the edge-cloud computing environment is a multi-objective problem, for which we employ deep reinforcement learning to find the optimal solution. To accomplish this, we formulate the problem as a Markov decision process and use a Double Deep Q-Network (DDQN) algorithm. Our DDQN-edge-cloud (DDQNEC) scheme dynamically makes offloading decisions by analyzing resource utilization, task constraints, and the current status of the edge-cloud network. Simulation results demonstrate that DDQNEC outperforms heuristic approaches in terms of resource utilization, task offloading, and task rejection."
10.1186/s13677-023-00480-0,Recommend what to cache: a simple self-supervised graph-based recommendation framework for edge caching networks,2023-07-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Deep Learning-based edge caching networks can accurately infer what to cache based on a user's historical content requests, thereby significantly relieving the burden of the backbone networks. However, the cold-start problem inherent in deep learning may limit the performance of history-based caching strategies. Due to the mobile and dynamic nature of wireless networks, base stations often lack sufficient data to accurately estimate the user's demands and cache the possible requested data. In this context, we adopt self-supervised learning (SSL) into the caching strategies and propose a Simple Self-supervised Graph-based Recommendation framework for edge caching networks (SimSGR). Specifically, we propose two new network layers: the Mixing layer and the Conversion layer. The former replaces the data augmentation of the SSL paradigm to avoid destroying the semantic loss, while the latter greatly simplifies the loss function, which helps to lighten the model structure and facilitates deployment on edge caching networks. Simulation results show that our model outperforms baseline algorithms that are sensitive to augmentation hyper-parameters, particularly when trained in a cold-start environment."
10.1186/s13677-023-00485-9,Latency and resource consumption analysis for serverless edge analytics,2023-07-19,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The serverless computing model, implemented by Function as a Service (FaaS) platforms, can offer several advantages for the deployment of data analytics solutions in IoT environments, such as agile and on-demand resource provisioning, automatic scaling, high elasticity, infrastructure management abstraction, and a fine-grained cost model. However, in the case of applications with strict latency requirements, the cold start problem in FaaS platforms can represent an important drawback. The most common techniques to alleviate this problem, mainly based on instance pre-warming and instance reusing mechanisms, are usually not well adapted to different application profiles and, in general, can entail an extra expense of resources. In this work, we analyze the effect of instance pre-warming and instance reusing on both application latency (response time) and resource consumption, for a typical data analytics use case (a machine learning application for image classification) with different input data patterns. Furthermore, we propose extending the classical centralized cloud-based serverless FaaS platform to a two-tier distributed edge-cloud platform to bring the platform closer to the data source and reduce network latencies."
10.1186/s13677-023-00482-y,Hyperparameter optimization method based on dynamic Bayesian with sliding balance mechanism in neural network for cloud computing,2023-07-19,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","['Hyperparameter optimization (HPO) of deep neural networks plays an important role of performance and efficiency of detection networks. Especially for cloud computing, automatic HPO can greatly reduce the network deployment cost by taking advantage of the computing power. Benefiting from its global-optimal search ability and simple requirements, Bayesian optimization has become the mainstream optimization method in recent years. However, in a non-ideal environment, Bayesian method still suffers from the following shortcomings: (1) when search resource is limited, it can only achieve inferior suboptimal results; (2) the acquisition mechanism cannot effectively balance the exploration of parameter space and the exploitation of historical data in different search stages. In this paper, we focused on the limited resources and big data provided by the cloud computing platform, took the anchor boxes of target detection networks as the research object, employed search resource as a restraint condition, and designed a dynamic Bayesian HPO method based on sliding balance mechanism. The dynamism of our method is mainly reflected in two aspects: (1) A dynamic evaluation model is proposed which uses the cross-validation mechanism to evaluate the surrogate model library and select the best model in real time; (2) A sliding balance mechanism is designed based on resource constraints to seek a balance between exploration and exploitation. We firstly augment the recommended samples of probability of improvement acquisition function by using k-nearest neighbor method, then introduce Hausdorff distance to measure the exploration value and match sampling strategy with resource utilization, which makes it slide smoothly with resource consumption to establish a dynamic balance of exploration to exploitation. The provided experiments show that our method can quickly and stably obtain better results under the same resource constraints compared with mature methods like BOHB.', 'Graphical Abstract', '']"
10.1186/s13677-023-00479-7,A truthful dynamic combinatorial double auction model for cloud resource allocation,2023-07-18,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Dynamic auction-based resource allocation models require little global price information, are decentralized and suitable for the distributed systems like cloud computing. For the cloud computing market, we proposed a Truthful Dynamic Combinatorial Double Auction (TDCDA) model to improve the social welfare and resource utilization. In our model, multiple cloud service providers and cloud users bid for various resources in a dynamic environment. We adopted a payment scheme to ensure truthfulness for all participants, which motivates bidders to reveal their true preferences. Since the combinatorial auction allocation with goal of economic efficiency is NP-hard, we developed a greedy mechanism to achieve the approximately efficient solution. Considering both parties’ interests and the resource scarcity, this model also ensures fairness and balances resource allocation. The proposed model is proven to be approximately efficient, incentive compatible, individually rational and budget-balanced. Simulation results show that the model not only achieves economic efficiency, but also improves resource allocation and meets resource needs for more cloud users."
10.1186/s13677-023-00477-9,An intelligent approach of task offloading for dependent services in Mobile Edge Computing,2023-07-18,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the growing popularity of Internet of Things (IoT), Mobile Edge Computing (MEC) has emerged for reducing the heavy workload at the multi-cloud core network by deploying computing and storage resources at the edge of network close to users. In IoT, services are data-intensive and event-driven, resulting in extensive dependencies among services. Traditional task offloading schemes face significant challenges in the IoT scenario with service dependencies. To this end, this paper proposes an intelligent approach for minimizing latency and energy consumption which jointly considers the task scheduling and resource allocation for dependent IoT services in MEC. Specifically, we establish the system model, communication model as well as computing model for performance evaluation by fully considering the dependent relationships among services, and an optimization problem is proposed for minimizing the delay and energy consumption simultaneously. Then, we design a layered scheme to deal with the service dependencies, and present detailed algorithms to intelligently obtain optimal task scheduling and resource allocation policies. Finally, simulation experiments are carried out to validate the effectiveness of the proposed scheme."
10.1186/s13677-023-00475-x,SEFSD: an effective deployment algorithm for fog computing systems,2023-07-15,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Fog computing aims to mitigate data communication delay by deploying fog nodes to provide servers in the proximity of users and offload resource-hungry tasks that would otherwise be sent to distant cloud servers. In this paper, we propose an effective fog device deployment algorithm based on a new metaheuristic algorithm–search economics–to solve the optimization problem for the deployment of fog computing systems. The term “effective” in this paper refers to that the developed algorithm can achieve better performance in terms of metrics such as lower latency and less resource usage. Compared with conventional metaheuristic algorithms, the proposed algorithm is unique in that it first divides the solution space into a set of regions to increase search diversity of the search and then allocates different computational resources to each region according to its potential. To verify the effectiveness of the proposed algorithm, we compare it with several classical fog computing deployment algorithms. The simulation results indicate that the proposed algorithm provides lower network latency and higher quality of service than the other deployment algorithms evaluated in this study."
10.1186/s13677-023-00484-w,Correction to: development of a cloud-assisted classification technique for the preservation of secure data storage in smart cities,2023-07-14,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems",
10.1186/s13677-023-00481-z,A conceptual architecture for simulating blockchain-based IoT ecosystems,2023-07-14,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Recently, the convergence between Blockchain and IoT has been appealing in many domains including, but not limited to, healthcare, supply chain, agriculture, and telecommunication. Both Blockchain and IoT are sophisticated technologies whose feasibility and performance in large-scale environments are difficult to evaluate. Consequently, a trustworthy Blockchain-based IoT simulator presents an alternative to costly and complicated actual implementation. Our primary analysis finds that there has not been so far a satisfactory simulator for the creation and assessment of blockchain-based IoT applications, which is the principal impetus for our effort. Therefore, this study gathers the thoughts of experts about the development of a simulation environment for blockchain-based IoT applications. To do this, we conducted two different investigations. First, a questionnaire is created to determine whether the development of such a simulator would be of substantial use. Second, interviews are conducted to obtain participants’ opinions on the most pressing challenges they encounter with blockchain-based IoT applications. The outcome is a conceptual architecture for simulating blockchain-based IoT applications that we evaluate using two research methods; a questionnaire and a focus group with experts. All in all, we find that the proposed architecture is generally well-received due to its comprehensive range of key features and capabilities for blockchain-based IoT purposes."
10.1186/s13677-023-00476-w,Mobile computing-enabled health physique evaluation in campus based on amplified hashing,2023-07-12,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the increasingly wide spread of COVID-19 pandemic, people’s various behavior activities are influenced more or less all over the world. For example, students in campus have to learn at home or in dormitory so as to avoid the attacks of the virus as much as possible. However, such a location distribution structure of student places a heavy burden on the monitoring and evaluating the sport physique of students in an effective and efficient way. Fortunately, the wide adoption of various mobile computing terminals (e.g., smart watches, mobile phones, etc.) and wireless communication technology makes it possible to know about the daily physique of students in a remote way. However, students’ health physique data are accumulated with time, which raises a challenge of quick data processing and cost-effective data scalability. Moreover, since the students are geographically distributed, we need to integrate their respective health physique data into a central cloud platform for more comprehensive data analysis and mining. However, the above data integration operations often involve student privacy. Motivated by the above two challenges, a mobile computing-aided health physique evaluation solution is brought forth in this paper, which is mainly based on a kind of amplified hashing technique. To prove the evaluation performances of the proposal, extensive experiments are designed to test the algorithm performances in terms of various evaluation metrics."
10.1186/s13677-023-00464-0,An ECC-based mutual data access control protocol for next-generation public cloud,2023-07-08,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Through the broad usage of cloud computing and the extensive utilization of next-generation public clouds, people can share valuable information worldwide via a wireless medium. Public cloud computing is used in various domains where thousands of applications are connected and generate numerous amounts of data stored on the cloud servers via an open network channel. However, open transmission is vulnerable to several threats, and its security and privacy are still a big challenge. Some proposed security solutions for protecting next-generation public cloud environments are in the literature. However, these methods may not be suitable for a wide range of applications in a next-generation public cloud environment due to their high computing and communication overheads because if security protocol is strengthened, it inversely impacts performance and vice versa. Furthermore, these security frameworks are vulnerable to several attacks, such as replay, denial-of-service (DoS), insider, server spoofing, and masquerade, and also lack strong user anonymity and privacy protection for the end user. Therefore, this study aims to design an elliptic curve cryptographic (ECC) based data access control protocol for a public cloud environment. The security mechanism of the proposed protocol can be verified using BAN (Burrows-Abadi-Needham) logic and ProVerif 2.03, as well as informally using assumptions and pragmatic illustration. In contrast, in the performance analysis section, we have considered the parameters such as the complexity of storage overheads, communication, and computation time. As per the numerical results obtained in the performance analysis section, the proposed protocol is lightweight, robust, and easily implemented in a practical next-generation cloud computing environment."
10.1186/s13677-023-00474-y,File processing security detection in multi-cloud environments: a process mining approach,2023-07-06,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud computing has gained popularity in recent years, but with its rise comes concerns about data security. Unauthorized access and attacks on cloud-based data, applications, and infrastructure are major challenges that must be addressed. While machine learning algorithms have improved intrusion detection systems in cloud data security, they often fail to consider the entire life cycle of file processing, making it difficult to detect certain issues, especially insider attacks. To address these limitations, this paper proposes a novel approach to analyzing data file processing in multi-cloud environments using process mining. By generating a complete file processing event log from a multi-cloud environment, the proposed approach enables detection from both control flow and performance perspectives, providing a deeper understanding of the underlying file processing in its full life cycle. Through our case study, we demonstrate the power and capabilities of process mining for file security detection and showcase its ability to provide further insights into file security in multi-cloud environments."
10.1186/s13677-023-00478-8,IoV data sharing scheme based on the hybrid architecture of blockchain and cloud-edge computing,2023-07-05,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Achieving efficient and secure sharing of data in the Internet of Vehicles (IoV) is of great significance for the development of smart transportation. Although blockchain technology has great potential to promote data sharing and privacy protection in the context of IoV, the problem of securing data sharing should be payed more attentions. This paper proposes an IoV data sharing scheme based on the hybrid architecture of blockchain and cloud-edge computing. Firstly, to improve protocol’s efficiency, a dual-chain structure empowered by alliance chain is introduced as the model architecture. Secondly, for the space problem characterized by data storage and security, we adopt distributed storage with the help of edge devices. Finally, to both ensure the efficiency of consensus protocol and protect the privacy of vehicles and owners simultaneously, we improve DPoS consensus algorithm to realize the efficient operation of the IoV data sharing model, which is closer to the actual needs of IoV. The comparison with other data sharing models highlights the advantages of this model, in terms of data storage and sharing security. It can be seen that the improved DPoS has high consensus efficiency and security in IoV."
10.1186/s13677-023-00472-0,Economical revenue maximization in mobile edge caching and blockchain enabled space-air-ground integrated networks,2023-06-29,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","In this paper, we study an edge caching and blockchain enabled space-air-ground integrated networking (SAGIN) network, where a low-earth-orbit (LEO) satellite serves as the content provider, and multiple edge caching enabled unmanned aerial vehicles (UAVs) will cache some contents to provide user equipments (UEs) with satisfactory content access services together with the satellite. Moreover, there’s a blockchain system that is deployed on UAVs, to provide the network with trust mechanism without requiring a centralized authority. From the standpoint of the operator, we intend to maximize the long-term averaged economical revenue by providing UEs with satisfactory and secure content access services. To achieve this purpose, we will jointly optimize the content placement of each UAV, content replacement when each UAV is full, the access control of each UE, and the blockchain deployment strategy about each UAV. the concept of queues in Lyapunov optimization is utilized to represent the backlog of edge equipment, ensuring the stability of virtual queues on UAVs and satellites, while satisfying the caching capacity constraints for content caching and blockchain deployment. Due to the tight coupling of optimization in each time slot and the variables within each time slot, our problem, which involves stochastic optimization and binary integer programming, is challenging to solve. To address this issue, we initially employ Lyapunov optimization theory to transform and decouple the problem into individual time-slot optimization problems. Subsequently, we utilize an effective heuristic algorithm called the fireworks algorithm to solve these individual optimization problems. However, the original fireworks algorithm cannot be directly applied to our problem due to its binary characteristics and inter-coupling constraints. Therefore, we have redesigned the explosion and mutation operations to adapt them to our specific problem. Simulation results demonstrate that our proposed algorithm outperforms other baseline algorithms."
10.1186/s13677-023-00473-z,VTGAN: hybrid generative adversarial networks for cloud workload prediction,2023-06-26,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Efficient resource management approaches have become a fundamental challenge for distributed systems, especially dynamic environment systems such as cloud computing data centers. These approaches aim at load-balancing or minimizing power consumption. Due to the highly dynamic nature of cloud workloads, traditional time series and machine learning models fail to achieve accurate predictions. In this paper, we propose novel hybrid VTGAN models. Our proposed models not only aim at predicting future workloads but also predicting the workload trend (i.e., the upward or downward direction of the workload). Trend classification could be less complex during the decision-making process in resource management approaches. Also, we study the effect of changing the sliding window size and the number of prediction steps. In addition, we investigate the impact of enhancing the features used for training using the technical indicators, Fourier transforms, and wavelet transforms. We validate our models using a real cloud workload dataset. Our results show that VTGAN models outperform traditional deep learning and hybrid models, such as LSTM/GRU and CNN-LSTM/GRU, concerning cloud workload prediction and trend classification. Our proposed model records an upward prediction accuracy ranging from $$95.4\%$$ 95.4 % to $$96.6\%$$ 96.6 % ."
10.1186/s13677-023-00468-w,Digital image watermarking using discrete cosine transformation based linear modulation,2023-06-24,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The proportion of multimedia traffic in data networks has grown substantially as a result of advancements in IT. As a result, it's become necessary to address the following challenges in protecting multimedia data: prevention of unauthorized disclosure of sensitive data, in addition to tracking down the leak's origin, making sure no alterations may be made without permission, and safeguarding intellectual property for digital assets. watermarking is a technique developed to combat this issue, which transfer secure data over the network. The main goal of invisible watermarking is a hidden exchange of data and a message from being discovered by a third party. The objective of this work is to develop a digital image watermarking using discrete cosine transformation based linear modulation. This paper proposed an invisible watermarking method for embedding information into the transformation domain for the grey scale images. This method used the embedding of a stego-text into the least significant bit (LSB) of the Discrete Cosine Transformation (DCT) coefficient by using a linear modulation algorithm. Also, a stego-text is embedded with different sizes ten times within images after embedding the stego-image immune to different kinds of attack, such as salt and pepper, rotation, cropping, and JPEG compression with different criteria. The proposed method is tested using four benchmark images. Also, to evaluate the embedding effect, PSNR, NC and BER are calculated. The outcomes show that the proposed approach is practical and robust, where the obtained results are promising and do not raise any suspicion. In addition, it has a large capacity, and its results are imperceptible, especially when 1bit/block is embedded."
10.1186/s13677-023-00458-y,Two-stage hybrid genetic algorithm for robot cloud service selection,2023-06-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Robot cloud service platform is a combination of cloud computing and robotics, providing intelligent cloud services for many robots. However, to select a cloud service that satisfys the robot’s requirements from the massive services with different QoS indicator in the cloud platform is an NP hard problem. In this paper, based on the cost model between the cloud platform, cloud services and cloud service robotics, we propose a two-stage service selection strategy, namely, candidate services selection stage according to the specific QoS requirements of service robots and final cost optimization stage. Additionally, with respect to optimizing the final cost for the model, we propose a Dynamic Vector Hybrid Genetic Algorithm (DVHGA) that is integrated with local and global search process as well as a three-phase parameter updating policy. Specifically, inspired by momentum optimization in deep learning, dynamic vector is integrated with DVHGA to modify the weights of QoS and ensure the reasonable allocation of resources. Moreover, we suggest a linear evaluation method for the service robots and the cloud platform concerning time and final cost at the same time, which could be expected to be used in the real application environment. Finally, the empirical results demonstrate that the proposed DVHGA outperforms other benchmark algorithms, i.e., DABC, ESWOA, GA, PGA and GA-PSO, in convergence rate, total final cost and evaluation score."
10.1186/s13677-023-00459-x,HGAT: smart contract vulnerability detection method based on hierarchical graph attention network,2023-06-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the widespread use of blockchain, more and more smart contracts are being deployed, and their internal logic is getting more and more sophisticated. Due to the large false positive rate and low detection accuracy of most current detection methods, which heavily rely on already established detection criteria, certain smart contracts additionally call for human secondary detection, resulting in low detection efficiency. In this study, we propose HGAT, a hierarchical graph attention network-based detection model, in order to address the aforementioned issues as well as the shortcomings of current smart contract vulnerability detection approaches. First, using Abstract Syntax Tree (AST) and Control Flow Graph, the functions in the smart contract are abstracted into code graphs (CFG). Then abstract each node in the code subgraph, extract the node features, utilize the graph attention mechanism GAT, splice the obtained vectors to form the features of each line of statements and use these features to detect smart contracts. To create test data and assess HGAT, we leverage the open-source smart contract vulnerability sample dataset. The findings of the experiment indicate that this method can identify smart contract vulnerabilities more quickly and precisely than other detection techniques."
10.1186/s13677-023-00457-z,Integrating request replication into FaaS platforms: an experimental evaluation,2023-06-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Function-as-a-Service (FaaS) is a popular programming model for building serverless applications, supported by all major cloud providers and many open-source software frameworks. One of the main challenges for FaaS providers is providing fault tolerance for the deployed applications, that is, providing the ability to mask failures of function invocations from clients. The basic fault tolerance approach in current FaaS platforms is automatically retrying function invocations. Although the retry approach is well suited for transient failures, it incurs delays in recovering from other types of failures, such as node crashes. This paper proposes the integration of a Request Replication mechanism in FaaS platforms and describes how this integration was implemented in Fission, a well-known, open-source platform. It provides a detailed experimental comparison of the proposed approach with the retry approach and an Active-Standby approach in terms of performance, availability, and resource consumption under different failure scenarios."
10.1186/s13677-023-00469-9,Development of a cloud-assisted classification technique for the preservation of secure data storage in smart cities,2023-06-21,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud computing is the most recent smart city advancement, made possible by the increasing volume of heterogeneous data produced by apps. More storage capacity and processing power are required to process this volume of data. Data analytics is used to examine various datasets, both structured and unstructured. Nonetheless, as the complexity of data in the healthcare and biomedical communities grows, obtaining more precise results from analyses of medical datasets presents a number of challenges. In the cloud environment, big data is abundant, necessitating proper classification that can be effectively divided using machine language. Machine learning is used to investigate algorithms for learning and data prediction. The Cleveland database is frequently used by machine learning researchers. Among the performance metrics used to compare the proposed and existing methodologies are execution time, defect detection rate, and accuracy. In this study, two supervised learning-based classifiers, SVM and Novel KNN, were proposed and used to analyses data from a benchmark database obtained from the UCI repository. Initially, intrusions were detected using the SVM classification method. The proposed study demonstrated how the novel KNN used for distance capacity outperformed previous studies. The accuracy of the results of both approaches is evaluated. The results show that the intrusion detection system (IDS) with a 98.98% accuracy rate produces the best results when using the suggested system."
10.1186/s13677-023-00467-x,Performance evaluation of multivariate statistical techniques using edge-enabled optimisation for change detection in activity monitoring,2023-06-19,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The monitoring of human activities using simple body worn sensors is an important and emerging area of research in machine learning. The sensors capture a large amount of data in a short period of Time in a relatively un-obtrusive manner. The sensor data might have different transitions to be used for deification of different user activities. Therefore, change point detection can be used to classify the transition from one underlying distribution to another. The automatic and accurate change point detection is not only used for different events, however, can also be used for generating real world datasets and responding to changes in patient vital signs in critical situation. Moreover, the huge amount of data can use the current state-of-the-art cloud and edge computing platforms to process the change detection locally and more efficiently. In this paper, we used multivariate exponentially weighted moving Average (MEWMA) for online change point detection. Additionally, genetic algorithm (GA) and particle swarm optimization (PSO) is used to automatically identify an optimal parameter set by maximizing the F-measure. The optimisation approach is implemented over an edge cloud platform so that the data can be processed locally and more accurately. Furthermore, we evaluate our approach against multivariate cumulative sum (MCUSUM) from state-of the-art in terms of different metric measures such as accuracy, precision, sensitivity, G-means and F-measure. Results have been evaluated based on real data set collected using accelerometer for a set of 9 distinct activities performed by 10 users for total period of 35 minutes with achieving high accuracy from 99.3% to 99.9% and F-measure up to 62.94%."
10.1186/s13677-023-00465-z,Fast DRL-based scheduler configuration tuning for reducing tail latency in edge-cloud jobs,2023-06-17,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Edge-cloud applications are rapidly prevailing in recent years and pose the challenge of using both resource-strenuous edge devices and elastic cloud resources under dynamic workloads. Efficient resource allocation on edge-cloud jobs via cluster schedulers (e.g. Kubernetes/Volcano scheduler) is essential to guarantee their performance, e.g. tail latency, and such allocation is sensitive to scheduler configurations such as applied scheduling algorithms and task restart/discard policy. Deep reinforcement learning (DRL) is increasingly applied to optimize scheduling decisions. However, DRL faces the conundrum of achieving high rewards at a dauntingly long training time (e.g. hours or days), making it difficult to tune the scheduler configurations online in accordance to dynamically changing edge-cloud workloads and resources. For such an issue, this paper proposes EdgeTuner, a fast scheduler configuration tuning approach that efficiently leverages DRL to reduce tail latency of edge-cloud jobs. The enabling feature of EdgeTuner is to effectively simulate the execution of edge-cloud jobs under different scheduler configurations and thus quickly estimate these configurations’ influence on job performance. The simulation results allow EdgeTuner to timely train a DRL agent in order to properly tune scheduler configurations in dynamic edge-cloud environment. We implement EdgeTuner in both Kubernetes and Volcano schedulers and extensively evaluate it on real workloads driven by Alibaba production traces. Our results show that EdgeTuner outperforms prevailing scheduling algorithms by achieving much lower tail latency while accelerating DRL training speed by an average of 151.63x."
10.1186/s13677-023-00466-y,Ensuring security in edge computing through effective blockchain node detection,2023-06-15,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The rapid development of blockchain technology has garnered increasing attention, particularly in the field of edge computing. It has become a significant subject of research in this area due to its ability to protect the privacy of data. Despite the advantages that blockchain technology offers, there are also security threats that must be addressed. Attackers may manipulate certain nodes in the blockchain network, which can result in tampering with transaction records or other malicious activities. Moreover, the creation of a large number of false nodes can be utilized to gain control and manipulate transaction records of the blockchain network, which can compromise the reliability and security of edge computing. This paper proposes a blockchain node detection method named $$T^2A2vec$$ T 2 A 2 v e c that provides a more secure, credible, and reliable solution to address these challenges. In order to achieve $$T^2A2vec$$ T 2 A 2 v e c , a transaction dataset that is evenly distributed in both space and time was collected. The transaction dataset is constructed as a transaction graph, where nodes represent accounts and edges describe transactions. BP neural network is used to extract account features, and a random walk strategy based on transaction time, type, and amount is used to extract transaction features. The obtained account features and transaction features are fused to obtain account representation. Finally, the obtained node representation is fed into different classifiers to identify malicious nodes."
10.1186/s13677-023-00460-4,A large-scale data security detection method based on continuous time graph embedding framework,2023-06-15,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Graph representation learning has made significant strides in various fields, including sociology and biology, in recent years. However, the majority of research has focused on static graphs, neglecting the temporality and continuity of edges in dynamic graphs. Furthermore, dynamic data are vulnerable to various security threats, such as data privacy breaches and confidentiality attacks. To tackle this issue, the present paper proposes a data security detection method based on a continuous-time graph embedding framework (CTDGE). The framework models temporal dependencies and embeds data using a graph representation learning method. A machine learning algorithm is then employed to classify and predict the embedded data to detect if it is secure or not. Experimental results show that this method performs well in data security detection, surpassing several dynamic graph embedding methods by 5% in terms of AUC metrics. Furthermore, the proposed framework outperforms other dynamic baseline methods in the node classification task of large-scale graphs containing 4321477 temporal information edges, resulting in a 10% improvement in the F1 score metric. The framework is also robust and scalable for application in various data security domains. This work is important for promoting the use of continuous-time graph embedding framework in the field of data security."
10.1186/s13677-023-00471-1,A survey of Kubernetes scheduling algorithms,2023-06-13,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","As cloud services expand, the need to improve the performance of data center infrastructure becomes more important. High-performance computing, advanced networking solutions, and resource optimization strategies can help data centers maintain the speed and efficiency necessary to provide high-quality cloud services. Running containerized applications is one such optimization strategy, offering benefits such as improved portability, enhanced security, better resource utilization, faster deployment and scaling, and improved integration and interoperability. These benefits can help organizations improve their application deployment and management, enabling them to respond more quickly and effectively to dynamic business needs. Kubernetes is a container orchestration system designed to automate the deployment, scaling, and management of containerized applications. One of its key features is the ability to schedule the deployment and execution of containers across a cluster of nodes using a scheduling algorithm. This algorithm determines the best placement of containers on the available nodes in the cluster. In this paper, we provide a comprehensive review of various scheduling algorithms in the context of Kubernetes. We characterize and group them into four sub-categories: generic scheduling, multi-objective optimization-based scheduling, AI-focused scheduling, and autoscaling enabled scheduling, and identify gaps and issues that require further research."
10.1186/s13677-023-00453-3,Comparative analysis of metaheuristic load balancing algorithms for efficient load balancing in cloud computing,2023-06-13,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Load balancing is a serious problem in cloud computing that makes it challenging to ensure the proper functioning of services contiguous to the Quality of Service, performance assessment, and compliance to the service contract as demanded from cloud service providers (CSP) to organizations. The primary objective of load balancing is to map workloads to use computing resources that significantly improve performance. Load balancing in cloud computing falls under the class of concerns defined as ""NP-hard"" issues due to vast solution space. Therefore it requires more time to predict the best possible solution. Few techniques can perhaps generate an ideal solution under a polynomial period to fix these issues. In previous research, Metaheuristic based strategies have been confirmed to accomplish accurate solutions under a decent period for those kinds of issues. This paper provides a comparative analysis of various metaheuristic load balancing algorithms for cloud computing based on performance factors i.e., Makespan time, degree of imbalance, response time, data center processing time, flow time, and resource utilization. The simulation results show the performance of various Meta-heuristic Load balancing methods, based on performance factors. The Particle swarm optimization method performs better in improving makespan, flow time, throughput time, response time, and degree of imbalance."
10.1186/s13677-023-00470-2,HVS-inspired adversarial image generation with high perceptual quality,2023-06-13,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Adversarial images are able to fool the Deep Neural Network (DNN) based visual identity recognition systems, with the potential to be widely used in online social media for privacy-preserving purposes, especially in edge-cloud computing. However, most of the current techniques used for adversarial attacks focus on enhancing their ability to attack without making a deliberate, methodical, and well-researched effort to retain the perceptual quality of the resulting adversarial examples. This makes obvious distortion observed in the adversarial examples and affects users’ photo-sharing experience. In this work, we propose a method for generating images inspired by the Human Visual System (HVS) in order to maintain a high level of perceptual quality. Firstly, a novel perceptual loss function is proposed based on Just Noticeable Difference (JND), which considered the loss beyond the JND thresholds. Then, a perturbation adjustment strategy is developed to assign more perturbation to the insensitive color channel according to the sensitivity of the HVS for different colors. Experimental results indicate that our algorithm surpasses the SOTA techniques in both subjective viewing and objective assessment on the VGGFace2 dataset."
10.1186/s13677-023-00462-2,A resource scheduling method for cloud data centers based on thermal management,2023-06-10,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the rapid growth of cloud computing services, the high energy consumption of cloud data centers has become a critical concern of the cloud computing society. While virtual machine (VM) consolidation is often used to reduce energy consumption, excessive VM consolidation may lead to local hot spots and increase the risk of equipment failure. One possible solution to this problem is to utilize thermal-aware scheduling, but existing approaches have trouble realizing the balance between SLA and energy consumption. This paper proposes a novel method to manage cloud data center resources based on thermal management (TM-VMC), which optimizes total energy consumption and proactively prevents hot spots from a global perspective. Its VM consolidation process includes four phases where the VMs scheduler uses an improved ant colony algorithm (UACO) to find appropriate target hosts for VMs based on server temperature and utilization status obtained in real-time. Experimental results show that the TM-VMC approach can proactively avoid data center hot spots and significantly reduce energy consumption while maintaining low Service Level Agreement (SLA) violation rates compared to existing mainstream VM consolidation algorithms with workloads from real-world data centers."
10.1186/s13677-023-00463-1,"An environment safety monitoring system for agricultural production based on artificial intelligence, cloud computing and big data networks",2023-06-08,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Monitoring the agricultural production environment is crucial for optimal crop growth and resource efficiency. Cloud Computing, Artificial Intelligence (AI), and Big Data have revolutionized traditional agriculture, promising improved output and product quality. The popularity of these technologies drives their application in safety monitoring. This system facilitates data collection and transmission among equipment, overcoming challenges of traditional systems like investment, costs, and maintenance. In this paper, cloud computing-based AI optimization technology and big data network were proposed to monitor the safety of the agricultural production environment, and the shortcomings of traditional distance vector hop (DV hop) positioning algorithms were analyzed in depth. RSSI (Received Signal Strength Indication) technology improved the traditional DV Hop location method. The paper analyses direct and indirect transmission for data transmission between WSN and cloud nodes and favors indirect transmission because it consumes less invalid energy. Finally, the article compares several evaluations of alternative algorithms for monitoring system performance, including data transmission reliability, data reception rate, and data delay. The experimental results in this paper showed that in the data reception rate test, the data reception rate of System 2 was 97% at the lowest and 99% at the highest, both exceeding 95%."
10.1186/s13677-023-00455-1,Managing the integration of teaching resources for college physical education using intelligent edge-cloud computing,2023-05-25,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","These days, colleges and universities have accumulated many resources in teaching and scientific research due to the acceleration of education information in China. However, many teaching resources are in short supply due to a lack of standardized resource construction and the closeness of management methods. Physical education significant teaching resources in Chinese colleges and universities must be utilized. If not integrated, it would seriously restrict the development of physical education in China. However, the traditional management of physical education teaching resources tends to worsen the data management, which is easy to cause the loss of physical education teaching resources data. With the development of the Internet of Things (IoT), cloud computing, and other technologies, intelligent edge cloud computing can ensure the integrity of physical education teaching resources and improve utilization. In this paper, cloud computing is used to manage physical education teaching resources in colleges and universities, and virtualization technology is used to research physical education teaching resources. Moreover, a resource scheduling method is proposed to ensure equal load distribution across various edge resources. The proposed strategy also provides increased utilization levels for computing resources. The comparison between the sports teaching resources under cloud computing and traditional sports teaching resources found that cloud resource management's sharing degree of teaching equipment has increased by 20.6% compared with conventional resource management. The sharing degree of courses has increased by 16.5%, and the utilization rate of sports venues has increased by 27.1% compared with traditional resource management. The utilization rate of sports film and television materials increased by 30.7%. Teachers and students benefit significantly from college and university teaching resources in the context of cloud computing. It demonstrates how cloud computing may assist colleges and universities in managing and integrating their teaching resources more effectively. The management and integration of college sports resources through cloud computing can promote the mutual exchange of college resources and have practical significance for the development of college education."
10.1186/s13677-023-00396-9,Optimal urban competitiveness assessment using cloud computing and neural network,2023-05-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","In the network economy domain, urban competitiveness refers to the comparison between cities in terms of competition and development. It is the ability to gain competitive advantage under different factors. The evaluation of urban competitiveness will help cities to learn from each other, and provides reference for the government to enhance urban competitiveness. Unlike various studies in the literature exploiting only the non-linear characteristics of urban competitiveness, this paper selects BP (Back Propagation) network as the main framework for evaluation. A Genetic Algorithm BP (GABP) network based on genetic optimization is utilized. The weights are optimized besides the crossover mutation of GA algorithm. To compensate the slow prediction in the stand-alone mode, this work proposes a MapReduce (MP) based method; MR-GABP via cloud computing. The model ensures effective urban competitiveness evaluation with improved convergence speed and threshold generation speed. The systematic experiments conducted verify effectiveness of the method while the results obtained reveal that performance of the method is better than the other methods in terms of accuracy and recall yielded as 95.1% and 92.6% respectively."
10.1186/s13677-023-00456-0,An accurate management method of public services based on big data and cloud computing,2023-05-17,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Information technology and resource grouping are developing rapidly thanks to governmental policies. Large-scale data mining is a hugely practical and scientific zone of “digital gold”. This is not only a modern-day problem, but it also necessitates the rapid growth of service of publics. As a result, the benefits of information can be fully realized, service efficiency improved, and the service gap closed. Cloud computing has gained popularity due to its quick accounting, limited memory requirements, and efficient resource distribution. As a result, big data was employed to analyze the existing state of cloud-based service of publics in this paper, and the fuzzy evaluation process was used to analyze the accurate monitoring. Eventually, service of public organization was improved, and the path to the accurate organization of the entire service of public process initiated by big data was developed. The design value and data entropy of cloud-based services of public were growing exponentially, according to fuzzy comprehensive assessment; the value of average of the design value of accurate organization of cloud-based government services was around 4.35, and the average importance of data entropy was around 0.98. Furthermore, the organization control and detailed efficiency of big data-driven precision organization of social public social welfare outperformed traditional precision monitoring of cloud-based service of publics; suggesting that the effects were 7% higher, and overall capacity was 9% higher than the compared techniques."
10.1186/s13677-023-00454-2,Cloud Enterprise Dynamic Risk Assessment (CEDRA): a dynamic risk assessment using dynamic Bayesian networks for cloud environment,2023-05-17,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud computing adoption has been increasing rapidly amid COVID-19 as organisations accelerate the implementation of their digital strategies. Most models adopt traditional dynamic risk assessment, which does not adequately quantify or monetise risks to enable business-appropriate decision-making. In view of this challenge, a new model is proposed in this paper for assignment of monetary losses terms to the consequences nodes, thereby enabling experts to understand better the financial risks of any consequence. The proposed model is named Cloud Enterprise Dynamic Risk Assessment (CEDRA) model that uses CVSS, threat intelligence feeds and information about exploitation availability in the wild using dynamic Bayesian networks to predict vulnerability exploitations and financial losses. A case study of a scenario based on the Capital One breach attack was conducted to demonstrate experimentally the applicability of the model proposed in this paper. The methods presented in this study has improved vulnerability and financial losses prediction."
10.1186/s13677-023-00439-1,Correction to: A cloud-oriented siamese network object tracking algorithm with attention network and adaptive loss function,2023-05-15,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems",
10.1186/s13677-023-00451-5,Verifiable attribute-based keyword search scheme over encrypted data for personal health records in cloud,2023-05-13,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Personal health record (PHR) is a medical model in which patients can upload their medical records and define the access control by themselves. Since the limited local storage and the development of cloud computing, many PHR services have been outsourced to the cloud. In order to ensure the privacy of electronic medical records, patients intend to encrypt their health records before uploading them. However, encrypted PHR can not be accessed directly and not be retrieved by legitimate users. To solve these issues, in this article we propose a new searchable encryption scheme with ciphertext-policy attributes, which achieves fine-grained access control and exact keyword search over encrypted PHRs. Moreover, in our proposed scheme, the receiver can verify the integrity of the search result that the cloud server returns. Finally, we simulate our scheme, and the experiments show that our scheme has high practicability for cloud-based healthcare systems and has high efficiency in aspects of keyword search and results verification."
10.1186/s13677-023-00450-6,Task offloading optimization mechanism based on deep neural network in edge-cloud environment,2023-05-11,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the rise of edge computing technology and the development of intelligent mobile devices, task offloading in the edge-cloud environment has become a research hotspot. Task offloading is also a key research issue in Mobile CrowdSourcing (MCS), where crowd workers collect sensed data through smart devices they carry and offload to edge-cloud servers or perform computing tasks locally. Current researches mainly focus on reducing resource consumption in edge-cloud servers, but fails to consider the conflict between resource consumption and service quality. Therefore, this paper considers the learning generation offloading strategy among multiple Deep Neural Network(DNN), proposed a Deep Neural Network-based Task Offloading Optimization (DTOO) algorithm to obtain an approximate optimal task offloading strategy in the edge-cloud servers to solve the conflict between resource consumption and service quality. In addition, a stack-based offloading strategy is researched. The resource sorting method allocates computing resources reasonably, thereby reducing the probability of task failure. Compared with the existing algorithms, the DTOO algorithm could balance the conflict between resource consumption and service quality in traditional edge-cloud applications on the premise of ensuring a higher task completion rate."
10.1186/s13677-023-00440-8,MRLCC: an adaptive cloud task scheduling method based on meta reinforcement learning,2023-05-10,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Task scheduling is a complex problem in cloud computing, and attracts many researchers’ interests. Recently, many deep reinforcement learning (DRL)-based methods have been proposed to learn the scheduling policy through interacting with the environment. However, most DRL methods focus on a specific environment, which may lead to a weak adaptability to new environments because they have low sample efficiency and require full retraining to learn updated policies for new environments. To overcome the weakness and reduce the time consumption of adapting to new environment, we propose a task scheduling method based on meta reinforcement learning called MRLCC. Through comparing MRLCC and baseline algorithms on the performance of shortening makespan in different environments, we can find that MRLCC is able to adapt to different environments quickly and has a high sample efficiency. Besides, the experimental results demonstrate that MRLCC can maintain a high utilization rate over all baseline algorithms after a few steps of gradient update."
10.1186/s13677-023-00442-6,Stochastic Gradient Descent long short-term memory based secure encryption algorithm for cloud data storage and retrieval in cloud computing environment,2023-05-09,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the increasing rise of distributed system technologies, one of the most pressing problems facing the digital world is ensuring the security of sensitive and confidential data during transport and storage, which is also regarded as one of the most critical difficulties facing cloud computing. Numerous techniques exist for enhancing data security in the cloud computing storage environment. Encryption is the most important method of data protection. Consequently, several accessible encryption strategies are utilized to provide security, integrity, and authorized access by employing modern cryptographic algorithms. Cloud computing is an innovative paradigm widely accepted as a platform for storing and analysing user data. The cloud is accessible via the internet, exposing the data to external and internal threats. Cloud Service Providers (CSPs) must now implement a secure architecture to detect cloud intrusions and safeguard client data from hackers and attackers. This paper combines Stochastic Gradient Descent long short-term memory (SGD-LSTM) and Blow Fish encryption to detect and prevent unauthorized cloud access. User registration, intrusion detection, and intrusion prevention are the three phases of the planned system. The SGD-LSTM classifier predicts cloud data access and prevents unauthorized cloud access. In the data access phase, cloud data access is managed by authenticating the authorized user with the Blowfish encryption algorithm. Comparing the proposed classifier to existing classifiers demonstrates that it detects abnormal access accurately. The experimental outcomes enhanced data security, which can be utilized to protect cloud computing applications. The experimental results of the suggested SGD-LSTM algorithm indicated a high level of protection, as well as a considerable improvement in security and execution speed when compared to algorithms that are often used in cloud computing."
10.1186/s13677-023-00449-z,An archetypal determination of mobile cloud computing for emergency applications using decision tree algorithm,2023-05-09,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Numerous users are experiencing unsafe communications due to the growth of big network mediums, where no node communication is detected in emergency scenarios. Many people find it difficult to communicate in emergency situations as a result of such communications. In this paper, a mobile cloud computing procedure is implemented in the suggested technique in order to prevent such circumstances, and to make the data transmission process more effective. An analytical framework that addresses five significant minimization and maximization objective functions is used to develop the projected model. Additionally, all mobile cloud computing nodes are designed with strong security, ensuring that all the resources are allocated appropriately. In order to isolate all the active functions, the analytical framework is coupled with a machine learning method known as Decision Tree. The suggested approach benefits society because all cloud nodes can extend their assistance in times of need at an affordable operating and maintenance cost. The efficacy of the proposed approach is tested in five scenarios, and the results of each scenario show that it is significantly more effective than current case studies on an average of 86%."
10.1186/s13677-023-00452-4,"Blockchain-based collaborative edge computing: efficiency, incentive and trust",2023-05-08,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The rise of 5G technology has driven the development of edge computing. Computation offloading is the key and challenging point in edge computing, which investigates offloading resource-intensive computing tasks from the user side to the cloud or edge side for processing. More consideration needs to be given to load balancing, user variability, and the heterogeneity of edge facilities in relevant research. In addition, most of the research around edge collaboration also revolves around cloud-side collaboration, which pays relatively little attention to the collaboration process between edge nodes, and the incentive and trust issues of the collaboration process need to be addressed. In this paper, we consider the impact of the user demand variability and the edge facility heterogeneity, then propose a method based on Vickrey-Clarke-Groves (VCG) auction theory to accommodate the edge demand response (EDR) process where the number of users and service facilities do not match. The method makes users’ bidding rules satisfy the Nash equilibrium and weakly dominant strategy, which can improve the load balancing of edge nodes, has positive significance in improving the edge resource utilization and reducing the system energy consumption. In particular, combined with blockchain, we further optimize the incentive and trust mechanism of edge collaboration and consider three scenarios: no collaboration, internal collaboration, and incentive collaboration. We also consider the impact of the user task’s transmission distance on the quality of experience (QoE). In addition, we illustrate the possible forking attack of blockchain in collaborative edge computing and propose a solution. We test the performance of the proposed algorithm on a real-world dataset, and the experimental results verify the algorithm’s effectiveness and the edge collaboration’s necessity."
10.1186/s13677-023-00446-2,Admission control policy and key agreement based on anonymous identity in cloud computing,2023-05-04,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud computing has completely revolutionized the concept of computing by providing users with always-accessible resources. In terms of computational, storage, bandwidth, and transmission costs, cloud technology offers its users an entirely new set of advantages and cost savings. Cross-cloud data migration, required whenever a user switches providers, is one of the most common issues the users encounter. Due to smartphones’ limited local storage and computational power, it is often difficult for users to back up all data from the original cloud servers to their mobile phones to upload and download the data to the new cloud provider. Additionally, the user must remember numerous tokens and passwords for different applications. In many instances, the anonymity of users who access any or all services provided by this architecture must be ensured. Outsourcing IT resources carries risks, particularly regarding security and privacy, because cloud service providers manage and control all data and resources stored in the cloud. However, cloud users would prefer that cloud service providers not know the services they employ or the frequency of their use. Consequently, developing privacy protections takes a lot of work. We devised a system of binding agreements and anonymous identities to address this problem. Based on a binding contract and admission control policy (ACP), the proposed model facilitates cross-cloud data migration by fostering cloud provider trust. Finally, Multi-Agent Reinforcement Learning Algorithm (MARL) is applied to identify and classify anonymity in the cloud by conducting various pre-processing techniques, feature selection, and dimensionality reduction."
10.1186/s13677-023-00447-1,Effect of using 5G and cloud computing environment for independent college english vocabulary learning,2023-05-03,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","An independent university refers to an institution of higher learning that has a bachelor's degree or higher education, and cooperates with social organizations other than state institutions or individuals to use non-state financial funds to establish and implement undergraduate education. While welcoming the arrival of the fifth-generation communication technology (5G) era, China's independent universities are also carrying out educational reforms to adapt to social development. 5G has created new opportunities for the development of students. Cloud computing refers to a system with extremely strong computing power formed through computer networks (mostly the Internet), which can store, collect relevant resources and configure them as needed to provide personalized services to users. Therefore, this paper studied and discussed the application of 5G and cloud computing in English vocabulary learning in independent universities. This paper explores the impact of 5G and cloud computing environments on independent college English vocabulary learning. With the assistance of 5G technology and a cloud computing environment, students' vocabulary mastery and application abilities have achieved certain improvements. Students' vocabulary learning can not only effectively break through the constraints of region and time, but also improve the interaction of learning and promote the communication and exchange between learners. This can not only deepen the understanding of vocabulary but also stimulate students' learning initiative and improve their learning ability and level. Given the current situation of English vocabulary acquisition, this paper used the fuzzy evaluation method to establish an evaluation model. The learning platform built by 5G and cloud computing technology has been utilized to study the effect of applying it to learners' vocabulary learning. In the questionnaire survey, 43.00% of the students are generally satisfied with the current English vocabulary teaching, 33.20% of the students think that their learning effect is average, 43.00% of the students think that the arrangement of teaching content is not reasonable, and 35.40% of the students think that the arrangement of class hours is generally reasonable. Before the experiment, the vocabulary of 68 and 65 students in the experimental group and the control group were between 1500–3500. Through the experiment, 67 students in the experimental group had vocabularies in the range of 3501–6500, while the vast majority of students in the control group still had vocabularies in the range of 1500–3500. In addition, through the experiment, the average total score of English test scores increased from 67 to 87 for the experimental group and from 69 to 72 for the control group. Based on the above data, it can be seen that the learning of English vocabulary through independent learning platforms built on 5G and cloud computing has a significant effect on the learning of English vocabulary of independent university students."
10.1186/s13677-023-00448-0,Innovation strategy design of public sports service governance based on cloud computing,2023-05-02,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Public sports service is an important component of the modern government construction service system. They play an important role in strengthening the national physique, improving people’s satisfaction with public services, and improving people’s welfare. With the implementation of the national sports strategy, the quality of sports has continuously improved, and significant progress has been made in the construction of sports facilities, sports service mechanisms, sports service projects, and sports service personnel. Due to the inclination of national development centers and differences in regional economic levels, the effectiveness and implementation of public sports service governance may be affected. In order to solve the current problems of unbalanced development of public SS and insufficient social security, this article used cloud computing to conduct an innovative design for public SS governance and surveyed the satisfaction of residents in the community with public SS governance under the innovative design. Through comparative experiments, it studied the service quality and implementation effect of cloud computing on public SS governance. Through experimental analysis, it was found that the service quality and implementation effect of the public SS governance innovation strategy were higher than the original public SS service system. The service quality of the edge computing public SS innovation system was about 10.6% higher than the original public SS system, and the implementation effect was about 8.1% higher than the original public SS system. Experiments have verified the feasibility of sports service innovation strategies under cloud computing."
10.1186/s13677-023-00443-5,Enhanced security using multiple paths routine scheme in cloud-MANETs,2023-04-29,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud Mobile Ad-hoc Networks (Cloud-MANETs) is a framework that can access and deliver cloud services to MANET users through their smart devices. MANETs is a pool of self-organized mobile gadgets that can communicate with each other with no support from a central authority or infrastructure. The main advantage of MANETs is its ability to manage mobility while data communication between different users in the system occurs. In MANETs, clustering is an active technique used to manage mobile nodes. The security of MANETs is a key aspect for the fundamental functionality of the network. Addressing the security-related problems ensures that the confidentiality and integrity of the data transmission is secure. MANETs are highly prone to attacks because of their properties.In clustering schemes, the network is broken down to sub-networks called clusters. These clusters can have overlapping nodes or be disjointed. An enhanced node referred to asthe Cluster Head (CH) is chosen from each set to overseetasks related to routing. It decreases the member nodes’ overhead and improvesthe performance of the system. The relationship between the nodes and CH may vary randomly, leading to re-associations and re-clustering in a MANET that is clustered. An efficient and effective routing protocol is required to allow networking and to find the most suitable paths between the nodes. The networking must be spontaneous, infrastructure-less, and provide end-to-end interactions. The aim of routing is the provision of maximum network load distribution and robust networks. This study focused on the creation of a maximal route between a pair of nodes, and to ensure the appropriate and accurate delivery of the packet. The proposed solution ensured that routing can be carried out with the lowest bandwidth consumption. Compared to existing protocols, the proposed solution had a control overhead of 24, packet delivery ratio of 81, the lowest average end-to-end delay of 6, and an improved throughput of 80,000, thereby enhancing the output of the network. Our result shows that multipath routing enables the network to identify alternate paths connecting the destination and source. Routing is required to conserve energy and for optimum bandwidth utilization."
10.1186/s13677-023-00428-4,Decentralized and scalable hybrid scheduling-clustering method for real-time applications in volatile and dynamic Fog-Cloud Environments,2023-04-28,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","A major challenge in Cloud-Fog settings is the scheduling of workflow applications with time constraints as the environment is highly volatile and dynamic. Furthermore, adding the complexities of handling IoT nodes, as the major owners of the workflow requests, renders the problem space even harder to address. This paper presents a hybrid scheduling-clustering method for addressing this challenge. The proposed lightweight, decentralized, and dynamic clustering algorithm is based on fuzzy inference with intrinsic support for mobility to form stable and well-sized clusters of IoT nodes while avoiding global clustering and recurrent re-clustering. The proposed distributed method uses Cloud resources along with clusters of mobile and inert Fog nodes to schedule time-constrained workflow applications with considering a proper balance between contradicting criteria and promoting scalability and adaptability. The Velociraptor simulator (version 0.6.7) has been used to throughtly examine and compare the proposed method in real workloads with two contemporary and noteworthy methods. The evaluation results show the superiority of the proposed method as the resource utilization is about 20% better and the schedule success rate is almost 21% better compared with the two other methods. Also, other parameters such as throughput and energy consumption have been studied and reported."
10.1186/s13677-023-00445-3,Load balancing scheduling mechanism for OpenStack and Docker integration,2023-04-28,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the development of cloud-edge collaborative computing, cloud computing has become crucial in data analysis and data processing. OpenStack and Docker are important components of cloud computing, and the integration of the two has always attracted widespread attention in industry. The scheduling mechanism adopted by the existing fusion solution uses a uniform resource weight for all containers, and the computing nodes resources on the cloud platform is unbalanced under differentiated resource requirements of the containers. Therefore, considering different network communication qualities, a load-balancing Docker scheduling mechanism based on OpenStack is proposed to meet the needs of various resources (CPU, memory, disk, and bandwidth) of containers. This mechanism uses the precise limitation strategy for container resources and a centralized strategy for container resources as the scheduling basis, and it generates exclusive weights for containers through a filtering stage, a weighing stage based on weight adaptation, and a non-uniform memory access (NUMA) lean stage. The experimental results show that, compared with Nova-docker and Yun, the proposed mechanism reduces the resource load imbalance within a node by 57.35% and 59.00% on average, and the average imbalance between nodes is reduced by 53.53% and 50.90%. This mechanism can also achieve better load balancing without regard to bandwidth."
10.1186/s13677-023-00441-7,Task grouping and optimized deep learning based VM sizing for hosting containers as a service,2023-04-25,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Containers as a service (CaaS) are a kind of services that permits the organization to handle the containers more effectively. Containers are lightweight, require less computing resources, portable, and facilitate better support for microservices. In the CaaS model, containers are deployed in virtual machines, and the virtual machine runs on the physical machine. The objective of this paper is to estimate the resource required by a VM to execute a number of containers. VM sizing is a directorial process where the system administrators have to optimize the allocated resources within the permitted virtualized space. In this work, the VM sizing is carried out using the Deep Convolutional Long Short Term Memory Network (Deep-ConvLSTM), where the weights are updated by Fractional Pelican Optimization (FPO) Algorithm. Here, the FPO is configured by hybridizing the concept of Fractional Calculus (FC) within the updated location of the Pelican Optimization Algorithm (POA). Moreover, the task grouping is done with Deep Embedded Clustering (DEC), where the grouping is established with respect to the various task parameters, such as task length, submission rate, scheduling class, priority, resource usage, task latency, and Task Rejection Rate (TRR). In addition, the performance analysis of VM sizing is done by taking 100, 200, 300, and 400 tasks. We got the best resource utilization of 0.104 with 300 tasks, a response time of 262ms with 100 tasks, and a TRR of 0.156 with 100 tasks and makespan of 0.5788 with 100 tasks."
10.1186/s13677-023-00411-z,Blockchain enabled zero trust based authentication scheme for railway communication networks,2023-04-24,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the introduction of emerging technologies such as cloud computing, the railway communication network has the characteristics of complex structure and blurred boundaries, which leads to a series of security threats including information leakage and malicious access. Specifically, the third-party cloud services are difficult to be supervised, and network traffic is untrustworthy. To ensure system security, we propose a zero-trust security model in this paper. Then, we introduce blockchain and Merkle tree to build a distributed identity storage scheme for guaranteeing reliable, confidential and efficient data updates, and improving authentication efficiency. Furthermore, the proxy was introduced for two-way authentication with cloud servers, so that internal and external threats could be counteracted. Moreover, reputation assessment mechanism has been adopted to reduce the possibility of nodes accessing malicious cloud services. Performance analysis demonstrated that the proposed security model is able to enhance the security, efficiency and stability of the system, and consequently can guarantee the safety and reliability of railway transportation."
10.1186/s13677-023-00437-3,Criminal law regulation of cyber fraud crimes—from the perspective of citizens’ personal information protection in the era of edge computing,2023-04-24,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Currently, cloud computing provides users all over the globe with Information and Communication Technology facilities that are utility-oriented. This technology is trying to drive the development of data center design by designing and building them as networks of cloud machines, enabling users to access and run the application from any part of the globe. Cloud computing provides considerable benefits to organizations by providing rapid and adaptable ICT software and hardware systems, allowing them to concentrate on creating innovative business values for the facilities they provide. The right to privacy of big data has acquired new definitions with the continued advancement of cloud computing, and the techniques available to protect citizens’ personal information under administrative law have managed to grow in a multitude. Because of the foregoing, internet fraud is a new type of crime that has emerged over time and is based on network technology. This paper analyzed and studied China’s internet fraud governance capabilities, and made a comprehensive evaluation of them using cloud computing technology and the Analytic Hierarchy Process (AHP). This paper discussed personal information security and the improvement of criminal responsibility from the perspective of citizens’ information security and designed and analyzed cases. In addition, this paper also analyzed and studied the ability of network fraud governance in the era of cloud computing. It also carried out a comprehensive evaluation and used the fuzzy comprehensive evaluation method to carry out the evaluation. A questionnaire survey was used to survey 100 residents in district X of city Z and district Y of the suburban area. Among the 100 people, almost all of them received scam calls or text messages, accounting for 99%, of which 8 were scammed. Among the people, more than 59.00% of the people expressed dissatisfaction with the government’s Internet fraud satisfaction survey. Therefore, in the process of combating Internet fraud, the government still needs to step up its efforts."
10.1186/s13677-023-00429-3,ReactiveFnJ: A choreographed model for Fork-Join Workflow in Serverless Computing,2023-04-24,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Function-as-a-Service (FaaS) is an event-based reactive programming model where functions run in ephemeral stateless containers for short duration. For building complex serverless applications, function composition is crucial to coordinate and synchronize the workflow of an application. Some serverless orchestration systems exist, but they are in their primitive state and do not provide inherent support for non-trivial workflows like, Fork-Join. To address this gap, we propose a fully serverless and scalable design model ReactiveFnJ for Fork-Join workflow. The intent of this work is to illustrate a design which is completely choreographed, reactive, asynchronous, and represents a dynamic composition model for serverless applications based on Fork-Join workflow. Our design uses two innovative patterns, namely, Relay Composition and Master-Worker Composition to solve execution time-out challenges. As a Proof-of-Concept (PoC), the prototypical implementation of Split-Sort-Merge use case, based on Fork-Join workflow is discussed and evaluated. The ReactiveFnJ handles embarrassingly parallel computations, and its design does not depend on any external orchestration services, messaging services, and queue services. ReactiveFnJ facilitates in designing fully automated pipelines for distributed data processing systems, satisfying the Serverless Trilemma in true essence. A file of any size can be processed using our effective and extensible design without facing execution time-out challenges. The proposed model is generic and can be applied to a wide range of serverless applications that are based on the Fork-Join workflow pattern. It fosters the choreographed serverless composition for complex workflows. The proposed design model is useful for software engineers and developers in industry and commercial organizations, total solution vendors and academic researchers."
10.1186/s13677-023-00444-4,Access control scheme based on blockchain and attribute-based searchable encryption in cloud environment,2023-04-18,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the rapid development of cloud computing technology, how to achieve secure access to cloud data has become a current research hotspot. Attribute-based encryption technology provides the feasibility to achieve the above goal. However, most of the existing solutions have high computational and trust costs. Furthermore, the fairness of access authorization and the security of data search can be difficult to guarantee. To address these issues, we propose a novel access control scheme based on blockchain and attribute-based searchable encryption in cloud environment. The proposed scheme achieves fine-grained access control with low computation consumption by implementing proxy encryption and decryption, while supporting policy hiding and attribute revocation. The encrypted file is stored in the IPFS and the metadata ciphertext is stored on the blockchain, which ensures data integrity and confidentiality. Simultaneously, the scheme enables the secure search of ciphertext keyword in an open and transparent blockchain environment. Additionally, an audit contract is designed to constrain user access behavior to dynamically manage access authorization. Security analysis proves that our scheme is resistant to chosen-plaintext attacks and keyword-guessing attacks. Theoretical analysis and experimental results show that our scheme has high computational and storage efficiency, which is more advantageous than other schemes."
10.1186/s13677-023-00438-2,Container cascade fault detection based on spatial–temporal correlation in cloud environment,2023-04-17,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Containers are light, numerous, and interdependent, which are prone to cascading fault, increasing the probability of fault and the difficulty of detection. Existing detection methods are usually based on a cascade fault model with traditional association analysis. The tradition model lacks consideration of the fault cascade history dimension and container space correlation dimension which results in a lower detection effect. And the imbalance of fault data in the cloud environment to the detection method to bring interference. Instead, this paper proposes a cascade fault detection method based on spatial–temporal correlation in cloud environment. First, the container cascade fault relationship model is constructed by extracting the spatial–temporal correlation from historical container faults. Second, based on dynamic feedback data sampling combined with ensemble learning, a container fault model learning method is designed to solve the imbalance of fault data. Then, a real-time container cascade fault detection mechanism for container cascade failure is proposed. The experimental results show that compared with the existing fault detection methods, the proposed method can effectively improve the detection accuracy, recall rate, and F_1 value."
10.1186/s13677-023-00436-4,Intelligent evaluation model of basketball teaching reliability based on swarm intelligence and edge computing,2023-04-17,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Campus basketball culture is gradually affecting students’ sports spirit and sports accomplishment. As for the evaluation of basketball teaching achievements, the method of specified items is generally used for testing, which is highly subjective. It’s completely teacher-led, and teachers make relevant evaluations of students’ basketball behavior. Teachers can’t be absolutely fair and just in the evaluation, because teacher evaluation can be affected by many factors, such as teachers’ mood on that day, teachers’ affection for students, and so on, the traditional way of teacher basketball evaluation is easy to cause negative emotional impact on some students, so that students have negative emotions on basketball activities, and even affect their sports quality, and finally affect their health. Based on the evaluation method of basketball teaching, this paper introduced a reliability intelligent evaluation model based on swarm intelligence and edge computing and used this model to evaluate students’ performance in basketball teaching classes. Moreover, this paper designed a related experiment, the experimental results showed that boys and girls in basketball level gap was more obvious. As far as dribbling skills were concerned, the highest score of boy A was 91 points, while the lowest score of girl C was 54 points. The gap was quite large. At the same time, the introduction results of the reliability intelligent evaluation model were studied by using the questionnaire survey method. As can be seen from the results of the questionnaire, the number of people who are very interested in basketball teaching activities is obviously high, and the number of people who are still not interested in the six activities is no more than 2. Through the change data of students’ interests and attitudes, it was proved that the reliability intelligent evaluation model could improve the students’ enthusiasm for learning basketball courses, thus improving their sports quality. This study provided a reference value for the application of swarm intelligence and edge computing in the intelligent evaluation model of basketball teaching reliability, and provided a direction for the future development of basketball teaching."
10.1186/s13677-023-00434-6,Robust-PAC time-critical workflow offloading in edge-to-cloud continuum among heterogeneous resources,2023-04-15,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Edge-to-cloud continuum connects and extends the calculation from edge side via network to cloud platforms, where diverse workflows go back and forth, getting executed on scheduled calculation resources. To better utilize the calculation resources from all sides, workflow offloading problems have been investigating lately. Most works focus on optimizing constraints like: latency requirements, resource utilization rate limits, and energy consumption bounds. However, the dynamics among the offloading environment have hardly been researched, which easily results in uncertain Quality of Service(QoS) on the user side. Any part of the workload change, resource availability change or network latency could incur dynamics in an offloading environment. In this work, we propose a robust PAC (probably approximately correct) offloading algorithm to address this dynamic issue together with optimization. We train an LSTM-based sequence-to-sequence neural network to learn how to offload workflows in edge-to-cloud continuum. Comprehensive implementations and corresponding comparison against state-of-the-art methods demonstrate the robustness of our proposed algorithm. More specifically, our algorithm achieves better offloading performance regarding dynamic heterogeneous offloading environment and faster adaptation to newly changed environments than fine-tuned state-of-the-art RL-based offloading methods."
10.1186/s13677-023-00435-5,A split-federated learning and edge-cloud based efficient and privacy-preserving large-scale item recommendation model,2023-04-14,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The combination of federated learning and recommender system aims to solve the privacy problems of recommendation through keeping user data locally at the client device during the model training session. However, most existing approaches rely on user devices to fully compute the deep model designed for the large-scale item recommendation; therefore, imposing high calculation and communication overheads on resource-constrained user devices. Consequently, achieving efficient federated recommendations across ubiquitous mobile devices remains an open research problem. To this end, in this paper we propose an efficient and privacy-preserving federated learning framework which is based on the cloud-edge collaboration for large-scale item recommendation called SpFedRec. In our method, to reduce the computation and communication cost of the federated two-tower model, a split learning approach is applied to migrate the item model from participants’ edge devices to the computationally powerful cloud side and compress item data while transmitting. Meanwhile, to enhance the feature representation, the Squeeze-and-Excitation network mechanism is used on the backbone model to optimize the perception of dominant features. Moreover, because the gradients transmitted contain private information about the user; therefore, we propose a multi-party circular secret-sharing chain based on secret sharing for better privacy protection. Extensive experiments using plausible assumptions on two real-world datasets demonstrate that our proposed method improves the average computation time and communication cost by 23% and 49%, respectively. Furthermore, the proposed model accomplishes comparable performance with other state-of-art federated recommendation models."
10.1186/s13677-023-00421-x,Resource allocation and network pricing based on double auction in mobile edge computing,2023-04-13,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","By transferring power-hungry huge data centers to lightweight Internet of Things (IoT) mobile devices, mobile edge computing (MEC) has completely changed the IoT. For MEC, to optimize economic gains and motivate profit-oriented entities, the joint resource allocation and network economics problem must be solved, and the joint issue is limited by local constraints, namely, the edge server only serves multiple nearby mobile devices, which is restricted by its available energy. The article studies the jointly issue of network economics and energy allocation in MEC, where mobile device apply for offloading at a purported bid and an edge server supplies its restricted serving at an asking price. In particular, this paper puts forward two dynamic pricing double auction strategies in the MEC system, i.e., a double auction according to the break-even mechanism (DABM) and a more practical double auction based on dynamic pricing mechanism (DADPM) to decide the matching between mobile devices and edge servers, and the pricing strategy for high-priced economic profit in the case of local restricts. Theoretical analysis shows that the proposed two algorithms have properties such as budget balance, individual rationality, economic benefit, authenticity. Extensive simulation experiments evaluate the efficiency of the system, and results verify that the proposed two schemes will greatly make better the economic benefits of MEC."
10.1186/s13677-023-00432-8,Correction: Improved wild horse optimization with levy fight algorithm for effective task scheduling in cloud computing,2023-04-12,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems",
10.1186/s13677-023-00427-5,NPR-LBN: next point of interest recommendation using large bipartite networks with edge and cloud computing,2023-04-11,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","During the last decades, tourism has been augmented worldwide through which the diversity of tourists’ interests is increased and is challenging to tackle with the traditional management system. Such challenges can be overcome by LBSNs (Location-Based Social Networks) such as Yelp, Foursquare, and Facebook which help to collect more personalized information close to tourists’ preferences/interests like check-ins, comments, and reviews. In this regard, solutions have been proposed to exploit the POI (Point of Interest) recommendation, but they failed to overcome sparsity and cold-start problems. Existing methods are also not focusing on important aspects, including geographical context, dynamics preferences and social influence, which are essential factors in POI recommendation. Therefore, this work tried to incorporate these factors and present a unified model using bipartite networks to learn users and POI dynamics. For this purpose, we have represented all the factors using eleven networks and combined them into a single latent space. In addition, Edge Computing processes data at the network's edge, reducing latency and bandwidth usage and enabling real-time and personalized recommendations. Furthermore, cloud computing could be used to store and process the large amounts of data collected from LBSNs, to support the proposed model's computational requirements and make it more accessible and scalable, allowing it to be easily used by tourism management systems worldwide. Experimental results show that our model outperforms state-of-the-art methods using real-world dataset in terms of accuracy and perform better against sparsity and cold-start problems."
10.1186/s13677-023-00430-w,Identification of encrypted and malicious network traffic based on one-dimensional convolutional neural network,2023-04-10,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The rapid advancement of the Internet has brought a exponential growth in network traffic. At present, devices deployed at edge nodes process huge amount of data, extract key features of network traffic and then forward them to the cloud server/data center. However, since the efficiency of mobile terminal devices in identifying and classifying encrypted and malicious traffic lags behind, how to identify network traffic more efficiently and accurately remains a challenging problem. We design a convolutional neural network model: One-dimensional convolutional neural network with hexadecimal data (HexCNN-1D) that combines normalized processing and attention mechanisms. By adding the attention mechanism modules Global Attention Block (GAB) and Category Attention Block (CAB), network traffic is classified and identified. By extracting effective load information from hexadecimal network traffic, our model can identify most categories of network traffic including encrypted and malicious traffic data. The experimental results show that the average accuracy is 98.8%. Our model can greatly improve the accuracy of network traffic data recognition."
10.1186/s13677-023-00433-7,Efficient online migration mechanism for memory write-intensive virtual machines,2023-04-06,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Online migration of virtual machines (VMs) is indispensable for system maintenance as it helps to achieve several resource management objectives such as load balancing, proactive fault tolerance, green operation, and resource management of data centers. The migration efficiency and reliability are two major challenges in the online migration of memory write-intensive VMs. For example, pre-copy migration transfers a large amount of data and takes a long time to migrate. This study proposes an efficient and reliable adaptive hybrid migration mechanism for memory write-intensive VMs. The mechanism optimizes the data transfer mode of the common migration method and improves the performance of conventional hybrid migration. First, the virtual machine (VM) memory data to be migrated are divided into dynamic and static data based on the bitmap marking method, and the migration efficiency is improved through parallel transmission based on different networks. Second, to accelerate the migration reliability, an iterative convergence factor is proposed to evaluate the current system load state and adaptively calculate the switching time of the migration mode for adaptive hybrid migration based on the convergence factor. Through adaptive hybrid migration can achieve migration completed successfully, shorten the post-copy migration duration, and minimize the impact on the performance of VMs. Finally, this paper implements the system prototype based on a kernel-based virtual machine (KVM), and experiments are performed using multiple memory write-intensive load VMs. The results show that the proposed migration algorithm can significantly improve migration performance and complete migration quickly to solve the pre-copy migration failure problem with a memory write-intensive load. Compared with the traditional hybrid migration with only one round of pre-copy, the proposed migration algorithm reduces the total migration time and transmits data by 23.2% and 26.7%, respectively."
10.1186/s13677-023-00431-9,A cloud-oriented siamese network object tracking algorithm with attention network and adaptive loss function,2023-04-04,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Aiming at solving the problems of low success rate and weak robustness of object tracking algorithms based on siamese network in complex scenes with occlusion, deformation, and rotation, a siamese network object tracking algorithm with attention network and adaptive loss function (SiamANAL) is proposed. Firstly, the multi-layer feature fusion module for template branch (MFFMT) and the multi-layer feature fusion module for search branch (MFFMS) are designed. The modified convolutional neural networks (CNN) are used for feature extraction through the fusion module to solve the problem of features loss caused by too deep network. Secondly, an attention network is introduced into the SiamANAL algorithm to calculate the attention of template map features and search map features, which enhances the features of object region, reduces the interference of background region, and improves the accuracy of the algorithm. Finally, an adaptive loss function combined with pairwise Gaussian loss function and cross entropy loss function is designed to increase inter-class separation and intra-class compactness of classification branches and improve the accuracy rate of classification and the success rate of regression. The effectiveness of the proposed algorithm is verified by comparing it with other popular algorithms on two popular benchmarks, the visual object tracking 2018 (VOT2018) and the object tracking benchmark 100 (OTB100). Extensive experiments demonstrate that the proposed tracker achieves competitive performance against state-of-the-art trackers. The success rate and precision rate of the proposed algorithm SiamANAL on OTB100 are 0.709 and 0.883, respectively. With the help of cloud computing services and data storage, the processing performance of the proposed algorithm can be further improved."
10.1186/s13677-023-00409-7,A blockchain-based SLA monitoring and compliance assessment for IoT ecosystems,2023-03-31,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","A Service Level Agreement (SLA) establishes the trustworthiness of service providers and consumers in several domains; including the Internet of Things (IoT). Given the proliferation of Blockchain technology, we find it compelling to reconsider the assumption of trust and centralised governance typically practised in SLA management including monitoring, compliance assessment, and penalty enforcement. Therefore, we argue that, such critical tasks should be operated by blockchain-based smart contracts in a non-repudiable manner beyond the influence of any SLA party. This paper envisions an IoT scenario wherein a firefighting station outsources end-to-end IoT operations to a specialised service provider. The contractual relationship between them is governed by an SLA which stipulates a set of quality requirements and violation consequences. The main contribution of this paper lies in designing, deploying and empirically experimenting a novel blockchain-based SLA monitoring and compliance assessment framework in the context of IoT. This is done by utilising Hyperledger Fabric (HLF), an enterprise-grade blockchain technology. Our work highlights a set of considerations and best practice at two sides, the IoT application monitoring-side and the blockchain-side. Moreover, it experimentally validates the reliability of the proposed monitoring approach, which collects relevant metrics from each IoT component and examines them against the quality requirements stated in the SLA. Finally, we propose a novel design for smart contracts at the blockchain-side, analyse and benchmark the performance, and demonstrate that the new design proves to successfully handle Multiversion Concurrency Control (MVCC) conflicts typically encountered in blockchain applications, while maintaining sound throughput and latency."
10.1186/s13677-023-00426-6,A convolutional neural network based online teaching method using edge-cloud computing platform,2023-03-28,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Teaching has become a complex essential tool for students’ abilities, due to their different levels of learning and understanding. In the traditional offline teaching methods, dance teachers lack a target for students ‘classroom teaching. Furthermore, teachers have limited time, so they cannot take full care of each student’s learning needs according to their understanding and learning ability, which leads to the polarization of the learning effect. Because of this, this paper proposes an online teaching method based on Artificial Intelligence and edge calculation. In the first phase, standard teaching and student-recorded dance learning videos are conducted through the key frames extraction through a deep convolutional neural network. In the second phase, the extracted key frame images were then extracted for human key points using grid coding, and the fully convolutional neural network was used to predict the human posture. The guidance vector is used to correct the dance movements to achieve the purpose of online learning. The CNN model is distributed into two parts so that the training occurs at the cloud and prediction happens at the edge server. Moreover, the questionnaire was used to obtain the students’ learning status, understand their difficulties in dance learning, and record the corresponding dance teaching videos to make up for their weak links. Finally, the edge-cloud computing platform is used to help the training model learn quickly form vast amount of collected data. Our experiments show that the cloud-edge platform helps to support new teaching forms, enhance the platform’s overall application performance and intelligence level, and improve the online learning experience. The application of this paper can help dance students to achieve efficient learning."
10.1186/s13677-023-00424-8,Data transmission reduction formalization for cloud offloading-based IoT systems,2023-03-28,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Computation offloading is the solution for IoT devices of limited resources and high-cost processing requirements. However, the network related issues such as latency and bandwidth consumption need to be considered. Data transmission reduction is one of the solutions aiming to solve network related problems by reducing the amount of data transmitted. In this paper, we propose a generalized formal data transmission reduction model independent of the system and the data type. This formalization is based on two main ideas: 1) Not sending data until a significant change occurs, 2) Sending a lighter size entity permitting the cloud to deduct the data captured by the IoT device without actually receiving it. This paper includes the mathematical representation of the model, general evaluation metrics formulas as well as detailed projections on real world use cases."
10.1186/s13677-023-00425-7,Novel secure data protection scheme using Martino homomorphic encryption,2023-03-27,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Globally, data maintenance and its security are vital tasks due to the rapid development of advanced technologies. Users now utilise Cloud computing technique and security techniques to preserve their data securely from intruders and hackers. Even yet, because of the technology's rapid advancement and inherent insecurity, attackers are conducting assaults on the cloud data. Hence, a homomorphic encryption technique was proposed based on Matrix Transformations with shifts, rotations, and transpositions of Binary converted ASCII values of each character in the plain text. For both encryption and decryption, symmetric cryptography employs the same secret key. The “avalanche effect” is a desirable feature of symmetric encryption in which two distinct keys generate separate cipher texts for the same message. As there are different conditions for the key, it helps to achieve this effect in this technique. The suggested algorithm's cryptanalysis reveals that it is more powerful than the existing encryption methods and resistant to a variety of attacks. So that an attacker cannot easily predict a plaintext through a statistical analysis."
10.1186/s13677-023-00413-x,Dynamic data collection algorithm based on mobile edge computing in underwater internet of things,2023-03-24,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The Underwater Internet of Things (UIoT) has emerged as one of the prominent technologies in the development of future ocean monitoring systems, where mobile edge elements (such as autonomous underwater vehicles (AUVs)) provide a promising method for the data collection from sensor nodes. However, as an important part of the UIoT, underwater wireless sensor networks (UWSNs) are severely affected by the underwater dynamic environment. For instance, node locations change continuously, which significantly increases the difficulty of data collection. To solve this problem, the concept of an inevitable communication space (ICS) is proposed. The ICS is calculated by analyzing the variation in the position of nodes and the communication range. Furthermore, an ICS-based dynamic data collection algorithm (ICS-DDCA) for UIoT is proposed to collect underwater data. This method utilizes the ICS instead of the initial location of the node for data collection to further improve the performance of the algorithm and shorten the data collection time. The simulation results demonstrate that compared with the energy-efficient data collection over AUV-assisted (EEDA) and data collection algorithms based on probabilistic neighborhood (PNCS-GHA), ICS-DDCA can effectively reduce the collection time, while ensuring the full completion of data collection."
10.1186/s13677-023-00418-6,Artificial intelligence and edge computing for teaching quality evaluation based on 5G-enabled wireless communication technology,2023-03-23,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud computing and artificial intelligence are now widely used for classroom teaching in higher learning institutes. The digital teaching supported to ICT technologies in colleges serves as a central point for the advancement of modern education; and has become as a mode of instruction and an approach to teaching. Digital teaching has emerged as a major driving force in the advancement of digital economy and digitization of education in colleges. In this paper, we investigate the movable information management system utilized in the digital teaching using edge computing and 5G wireless communication technology. Furthermore, we explain the idea of a mobile data scheme and presents a teaching platform based on the edge computing and 5G-enabled wireless communication technology. The main objective of this work is to develop a digital teaching framework for college students that, in fact, enables digital teaching, the collection, and incorporation of teaching information, the provision of modern education, and sharing of resources. Cutting-edge technology advancements in the educational platform have the potential to improve 5G communication. To implement the cutting-edge technology, all types of technological devices, smart devices, and gadgets from the Internet of Things (IoT) platform are used. We evaluated the proposed system through reasonable assumptions and numerical simulations. The experimental results reveal that the suggested system has significantly improved the teaching efficiency with which digital teaching management is managed in colleges. Moreover, the edge and 5G technology can significantly improve the system performance, in terms of response time, that can be as high as 11.45% when compared to non-cloud based approaches."
10.1186/s13677-023-00419-5,Edge computing-based digital management system of game events in the era of Internet of Things,2023-03-23,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the great development of Internet of Things (IoT) and edge computing, the development of sports activities depends on the development of information technology and it is inevitable to pay attention to the combination and optimization of resources. The combination of IoT and edge computing will be critical in sports activities. This paper elaborates on the application of network skill in sports event information management, that is, through the effective gathering of sports event data, to realize the use of sports event information, to achieve the purpose of information and digitization. Furthermore, the goal is to investigate the effect of sports event in the era of IoT. The impact of sports events on the economy and culture of the hosting city is investigated using IoT concept of edge computing. By analyzing the advantages and disadvantages of traditional centralized optimization method, we present a series of performance indicators and utility functions and show that the method is effective and achieves the optimal purpose. Through vital research, it is found that with the development of the edge computing and IoT industry, the scale of sports events is constantly expanding. By 2019, there has been a scale of 1,271 billion yuan. An increase of 981 billion yuan, compared with 290 billion yuan in 2013. Therefore, the use of the IoT technology in combination with edge computing to manage sports events will greatly encourage the expansion of sports activities. Furthermore, the holding of sporting events reflects a city’s overall strength and enhances the city’s exposure and fame. The investigation offers a certain reference point for cities looking to increase their influence through events."
10.1186/s13677-023-00416-8,IoT trust and reputation: a survey and taxonomy,2023-03-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","IoT is one of the fastest-growing technologies and it is estimated that more than a billion devices would be utilised across the globe by the end of 2030. To maximise the capability of these connected entities, trust and reputation among IoT entities is essential. Several trust management models have been proposed in the IoT environment; however, these schemes have not fully addressed the IoT devices’ features, such as device’s role, device type and its dynamic behavior in a smart environment. As a result, traditional trust and reputation models are insufficient to tackle these characteristics and uncertainty risks while connecting nodes to the network. Whilst continuous study has been carried out and various articles suggest promising solutions in constrained environments, research on trust and reputation is still at its infancy. In this paper, we carry out a comprehensive literature review on state-of-the-art research on the trust and reputation of IoT devices and systems. Specifically, we first propose a new structure, namely a new taxonomy, to organise the trust and reputation models based on the ways trust is managed. The proposed taxonomy comprises of traditional trust management-based systems and artificial intelligence-based systems, and combine both the classes which encourage the existing schemes to adapt these emerging concepts. This collaboration between the conventional mathematical and the advanced ML models result in design schemes that are more robust and efficient. Then we drill down to compare and analyse the methods and applications of these systems based on community-accepted performance metrics,e.g. scalability, delay, cooperativeness and efficiency. Finally, built upon the findings of the analysis, we identify and discuss open research issues and challenges, and further speculate and point out future research directions."
10.1186/s13677-023-00420-y,Enhancement of an IoT hybrid intrusion detection system based on fog-to-cloud computing,2023-03-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Nowadays, with the proliferation of internet of things-connected devices, the scope of cyber-attacks on the internet of things has grown exponentially. So, it makes it a necessity to develop an efficient and accurate intrusion detection system that should be fast, dynamic, and scalable in an internet of things environment. On the other hand, Fog computing is a decentralized platform that extends Cloud computing to deal with the inherent issues of the Cloud computing. As well, maintaining a high level of security is critical in order to ensure secure and reliable communication between Fog nodes and internet of things devices. To address this issue, we present an intrusion detection method based on artificial neural networks and genetic algorithms to efficiently detect various types of network intrusions on local Fog nodes. Through this approach, we applied genetic algorithms to optimize the interconnecting weights of the network and the biases associated with each neuron. Therefore, it can quickly and effectively establish a back-propagation neural network model. Moreover, the distributed architecture of fog computing enables the distribution of the intrusion detection system over local Fog nodes with a centralized Cloud, which achieves faster attack detection than the Cloud intrusion detection mechanism. A set of experiments were conducted on the Raspberry Pi4 as a Fog node, based on the UNSW-NB15 and ToN_IoT data sets for binary-class classification, which showed that the optimized weights and biases achieved better performance than those who used the neural network without optimization. The optimized model showed interoperability, flexibility, and scalability. Furthermore, achieving a higher intrusion detection rate through decreasing the neural network error rate and increasing the true positive rate is also possible. According to the experiments, the suggested approach produces better outcomes in terms of detection accuracy and processing time. In this case, the proposed approach achieved an 16.35% and 37.07% reduction in execution time for both data sets, respectively, compared to other state-of-the-art methods, which enhanced the acceleration of the convergence process and saved processing power."
10.1186/s13677-023-00423-9,Service composition considering energy consumption of users and transferring files in a multicloud environment,2023-03-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","In the internet and cloud environment, service composition is always used to enhance the function and processing ability of clouds. Those clouds work together for a user and provide different functions. A service request may involve multiple clouds. The past work focuses on the method of service composition and ignores the energy composition when files are transferred between clouds, including the energy consumption for transferring files (sending files from the user to the cloud and receiving files from the cloud to the user) of the user. The paper models the service composition in a multicloud environment. Based on those models, we use the GA (genetic algorithm) algorithm (GA-C) to solve the service composition problem with multiple targets in a multicloud environment. Simulation results show that the GA-C can: (1) reduce the average number of involved clouds and the energy consumption between clouds, and (2) reduce the energy consumption of the user and the failure rate of service composition."
10.1186/s13677-023-00422-w,Securing medical image privacy in cloud using deep learning network,2023-03-21,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The worldwide usage of Internet of Things (IoT) applications enhanced the utilization of consumer devices, such as smartphones, computers, screening equipment used in hospitals that merely rely on imaging techniques. Numerous images got generated over the cloud platform in a daily basis ad create storage complexity. On the other hand, securing the data stored in the cloud is important. Instead of storing large amount of data into the cloud, lightweight dynamic processing of data suppresses the complex issues in cloud security. Here secure cloud-based image processing architecture is discussed. Privacy preserving medical data communication is considered as the specific research scope. Cryptographic technique used to encode the original data and decode the data at the other end is currently in usage as conventional design. Providing privacy to the medical records through adding noise and denoising the same records is the proposed idea. The proposed work is keenly focused on creating a light weight cloud architecture that communicates the medical data effectively with privacy perseverance using deep learning technique. In the proposed system, the design of an efficient image denoising scheme with a hybrid classification model is created to ensure reliable and secure communication. Deep learning algorithms merged to form a Pseudo-Predictive Deep Denoising Network (PPDD). The proposed system's benefit is ensuring added security in Dark Cloud using a newly structured algorithm. The original data is packed in the Deep cloud using the Gaussian noise act as a key. The complete packing and unpacking of medical data is encapsulated by the transformed images. Over the cloud premise, the data is highly secured and invisible to the malicious users. To reduce the storage complexity, the dynamic data is unpacked and denoise process is applied at the edge devices. During the authorized access period alone, the data is decrypted and accessible at the edge nodes. The maximum process is dynamically happen in the cloud without depending on the storage boundary. The performance of proposed PPDD network model is evaluated through Signal to noise ratio (SNR), Similarity index (SI),Error Rate(ER) and Contrast to noise ratio(CNR). The proposed architecture is comparatively validated with existing state-of-art approach."
10.1186/s13677-023-00417-7,Cloud-edge data encryption in the internet of vehicles using Zeckendorf representation,2023-03-17,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","['Cloud-edge data security is a key issue in the internet of vehicles (IoV), as the potential for data breaches increases as more vehicles are connected. As vehicles become smarter and more connected, the risk of unauthorized access to the data generated by the vehicles also increases. Data encryption is a highly effective security measure that is widely used to protect the IoV from malicious actors. By encrypting data, it becomes virtually impossible for unauthorized individuals to access the information. This ensures that only the intended parties can access the data, allowing for secure communication between cloud and edge. Data encryption is a cost-effective and reliable security measure that is essential for any organization that relies on the IoV. The IoV is characterized by the large volume of data that is exchanged between devices in cloud and edge. This necessitates the use of a strong encryption method, such as stream ciphering, which is particularly well-suited to this type of environment. Stream ciphering provides the highest levels of security, making it the ideal choice for securing data transmission in the IoV. Many stream ciphering algorithms use bitwise exclusive or (XOR) to encrypt the data stream, so the core is the generation of a pseudo-random key stream. This paper proves that the probability of the number 1 appearing in the middle part of the Zeckendorf representation is constant, which can be used to generate pseudo-random key stream sequences. The pseudo-random sequence generated by the linear feedback shift register (LSFR) is periodic, and the key sequence will be duplicated. The logistic chaos (LC) sequence is too sensitive to the disturbance of initial value, and its stability is poor. In this paper, our proposed ZPKG (key generator based on Zeckendorf presentation) algorithm solves these two main problems in stream ciphering. The generated key sequence not only has strong randomness, but also is infinitely long, and it is robust to the minor disturbance of the initial value.', 'Graphical Abstract', '']"
10.1186/s13677-023-00412-y,An artificial intelligence lightweight blockchain security model for security and privacy in IIoT systems,2023-03-16,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The Industrial Internet of Things (IIoT) promises to deliver innovative business models across multiple domains by providing ubiquitous connectivity, intelligent data, predictive analytics, and decision-making systems for improved market performance. However, traditional IIoT architectures are highly susceptible to many security vulnerabilities and network intrusions, which bring challenges such as lack of privacy, integrity, trust, and centralization. This research aims to implement an Artificial Intelligence-based Lightweight Blockchain Security Model (AILBSM) to ensure privacy and security of IIoT systems. This novel model is meant to address issues that can occur with security and privacy when dealing with Cloud-based IIoT systems that handle data in the Cloud or on the Edge of Networks (on-device). The novel contribution of this paper is that it combines the advantages of both lightweight blockchain and Convivial Optimized Sprinter Neural Network (COSNN) based AI mechanisms with simplified and improved security operations. Here, the significant impact of attacks is reduced by transforming features into encoded data using an Authentic Intrinsic Analysis (AIA) model. Extensive experiments are conducted to validate this system using various attack datasets. In addition, the results of privacy protection and AI mechanisms are evaluated separately and compared using various indicators. By using the proposed AILBSM framework, the execution time is minimized to 0.6 seconds, the overall classification accuracy is improved to 99.8%, and detection performance is increased to 99.7%. Due to the inclusion of auto-encoder based transformation and blockchain authentication, the anomaly detection performance of the proposed model is highly improved, when compared to other techniques."
10.1186/s13677-023-00414-w,Efficient lattice-based revocable attribute-based encryption against decryption key exposure for cloud file sharing,2023-03-11,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud file sharing (CFS) has become one of the important tools for enterprises to reduce technology operating costs and improve their competitiveness. Due to the untrustworthy cloud service provider, access control and security issues for sensitive data have been key problems to be addressed. Current solutions to these issues are largely related to the traditional public key cryptography, access control encryption or attribute-based encryption based on the bilinear mapping. The rapid technological advances in quantum algorithms and quantum computers make us consider the transition from the tradtional cryptographic primitives to the post-quantum counterparts. In response to these problems, we propose a lattice-based Ciphertext-Policy Attribute-Based Encryption(CP-ABE) scheme, which is designed based on the ring learing with error problem, so it is more efficient than that designed based on the learing with error problem. In our scheme, the indirect revocation and binary tree-based data structure are introduced to achieve efficient user revocation and dynamic management of user groups. At the same time, in order to further improve the efficiency of the scheme and realize file sharing across enterprises, the scheme also allows multiple authorities to jointly set up system parameters and manage distribute keys. Furthermore, by re-randomizing the user’s private key and update key, we achieve decryption key exposure resistance(DKER) in the scheme. We provide a formal security model and a series of security experiments, which show that our scheme is secure under chosen-plaintext attacks. Experimental simulations and evaluation analyses demonstrate the high efficiency and practicality of our scheme."
10.1186/s13677-023-00397-8,Enriching computing simulators by generating realistic serverless traces,2023-03-11,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Serverless computing is stepping forward to provide a cloud environment that mainly focuses on managing infrastructure, resources and configurations on the behalf of a user. Research in this field can’t rely on commercial providers such as AWS and Azure, as their inflexibility and cost often limits the required levels of reproducibility and scalability. Therefore, simulators have been opted as an alternative solution by the research community. They offer a reduced-cost and easy-setup environment. To get respectable precision, simulators use real traces collected and offered by commercial providers. These traces represent comprehensive information of executed tasks that reflect user behaviour. Due to serverless computing’s recency, typical workload traces employed by IaaS simulators are not well adoptable to the new computing model. In this paper, we propose an approach for generating realistic serverless traces. We enhance our previous generator approach that was based on the Azure Functions dataset. Our new, genetic algorithm based approach improves the statistical properties of the generated traces. We also enabled arbitrary scaling of the workload, while maintaining real users’ behaviour. These advances further support reproducibility in the serverless research community. We validated the results of our generator approach using the coefficient of determination ( $$R^2$$ R 2 ), which shows that our generated workload closely matches the original dataset’s characteristics in terms of execution time, memory utilisation as well as user participation percentage. To demonstrate the benefits of the reusability of the generated traces, we applied them with a diverse set of simulators and shown that they offer reproducible results independently of the simulator used."
10.1186/s13677-023-00395-w,Cost-based hierarchy genetic algorithm for service scheduling in robot cloud platform,2023-03-09,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Service robot cloud platform is effective method to improve the intelligence of robots. An efficient cloud service scheduling algorithm is the basis of ensuring service quality and platform concurrency. In this paper, Hierarchy Genetic Algorithm of robot service(RHGA) is presented to solve the scheduling problem with the crucial constraints. Firstly, the limitations and attributes of the cloud service robots and cloud services are presented and boiled down to an important optimization goal. Secondly, three factors (i.e. evolutionary factor, hunting factor and parent similarity) are integrated with RHGA to promote the efficiency of small-scale service invocations and improve the performance of large-scale service invocations on the platform. Finally, a series of experiments are conducted on several service scheduling algorithms, including four traditional efficient algorithms and two state-of-art algorithms. The experimental results demonstrate that the RHGA can enhance the performance on small-scale service scheduling and ensure its excellent ability in large-scale service scheduling. Moreover, the empirical studies also prove that our proposal has a better performance in service scheduling completion time and cost-savings with comparison to other methods."
10.1186/s13677-023-00415-9,Cross-domain coordination of resource allocation and route planning for the edge computing-enabled multi-connected vehicles,2023-03-08,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Multi-access edge computing (MEC) has unique interests in processing intensive and delay-critical computation tasks for vehicular application through computation offloading. However, due to the spatial inhomogeneity and dynamic mobility of connected vehicles (CVs), the edge servers (ESs) must dynamically adjust their resource allocation schemes to effectively provide computation offloading services for CVs. In this case, we propose a MEC framework supporting the collaboration of CVs, and incorporate digital twins (DTs) into wireless network to mitigate the unreliable long-distance communication between CVs and ESs. To solve the contradiction between the task change requirements of CVs and ES resources, we proactively balance the computation resources load of ESs by appropriately cooperative route planning of CVs, and achieve cross-domain load balancing between traffic flow and edge cloud resources domains. Furthermore, we jointly formulate route planning and resource allocation to balance the travel and service time delay by considering the mobility of CVs, distributed resources of ESs and the deadline sensitive vehicular tasks comprehensively. Besides, considering the coupled relationship between route planning and resource allocation, an alternating optimization algorithm is proposed to solve the formulated problem. we decompose it into two sub-problems. Firstly, a reinforcement learning method is used to optimize the route planning of CVs with fixed resource allocation. Then, an online learning and iterative algorithm is used to optimize the resource allocation strategy of edge cloud with fixed route selection. In order to demonstrate that our suggested scheme is more effective than other comparison schemes, a comprehensive series of experiments are conducted."
10.1186/s13677-022-00378-3,SeisDeNet: an intelligent seismic data Denoising network for the internet of things,2023-03-08,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Deep learning (DL) has attracted tremendous interest in various fields in last few years. Convolutional neural networks (CNNs) based DL architectures have been successfully applied in computer vision, medical image processing, remote sensing, and many other fields. A recent work has proved that CNNs based models can also be used to handle geophysical problems. Due to noises in seismic signals acquired by geophone equipment this kind of important multimedia resources cannot be effectively utilized in practice. To this end, from the perspective of seismic exploration informatization, this paper takes informatization data in seismic signal acquisition and energy exploration field using cutting-edge technologies such as Internet of things and cloud computing as the research object, presenting a novel CNNs based seismic data denoising (SeisDeNet) architecture is suggested. Firstly, a multi-scale residual dense (MSRD) block is built to leverage the characteristics of seismic data. Then, a deep MSRD network (MSRDN) is proposed to restore the noisy seismic data in a coarse-to-fine manner by using cascading MSRDs. Additionally, the denoising problem is formulated into predicting transform-domain coefficients, by which noises can be further removed by MSRDNs while richer structure details are preserved comparing with the results in spatial domain. By using synthetic seismic records, public SEG and EAGE salt and overthrust seismic model and real field seismic data, the proposed method is qualitatively and quantitatively compared with other leading edge schemes to evaluate it performance, and some results shows that the proposed scheme can produce data with higher quality evaluation while maintaining far more useful data comparing with other schemes. The feasibility of this approach is confirmed by the denoising results, and this approach is shown to be promising in suppressing the seismic noise automatically."
10.1186/s13677-023-00407-9,A lightweight convolutional neural network based on dense connection for open-pit coal mine service identification using the edge-cloud architecture,2023-03-07,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Remote sensing is an important technical tool for rapid detection of illegal mining behavior. Due to the complex features of open-pit coal mines, there are few studies about automatic extraction of open-pit coal mines. Based on Convolutional Neural Network and Dense Block, we propose a lightweight densely connected network-AD-Net for the extraction of open-pit coal mining areas from Sentinel-2 remote sensing images, and construct three sample libraries of open-pit coal mining areas in north-central Xinzhou City, Shanxi Province. The AD-Net model consists of two convolutional layers, two pooling layers, a channel attention module, and a Dense Block. The two convolutional layers greatly reduce the complexity of the model, and the Dense Block enhances the feature propagation while reducing the parameter computation. The application is designed in different modules that runs independently on different machines and communicate with each other. Furthermore, we create and build a unique remote sensing image service system that connects a remote datacentre and its associated edge networks, employing the edge-cloud architecture. While the datacentre acts as the cloud platform and is in charge of storing and processing the original remote sensing images, the edge network is largely utilised for caching, predicting, and disseminating the processed images. First, we find out the optimal optimizer and the optimal size of the input image by extensive experiments, and then we compare the extraction effect of AD-Net with AlexNet, VGG-16, GoogLeNet, Xception, ResNet50, and DenseNet121 models in the study area. The experimental results show that the combination of NIR, red, green, and blue band synthesis is more suitable for the extraction of the open-pit coal mine, and the OA and Kappa of AD-Net reach 0.959 and 0.918 respectively, which is better than other models and well balances the classification accuracy and running speed. With this design of edge-cloud, the proposed system not only evenly distributes the strain of processing activities across the edges but also achieves data efficiency among them, reducing the cost of data transmission and improving the latency."
10.1186/s13677-023-00410-0,FSPLO: a fast sensor placement location optimization method for cloud-aided inspection of smart buildings,2023-03-06,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the awakening of health awareness, people are raising a series of health-related requirements for the buildings they live in, with a view to improving their living conditions. In this context, BIM (Building Information Modeling) makes full use of cutting-edge theories and technologies in many domains such as health, environment, and information technology to provide a new way for engineers to design and build various healthy and green buildings. Specifically, sensors are playing an important role in achieving smart building goals by monitoring the surroundings of buildings, objects and people with the help of cloud computing technology. In addition, it is necessary to quickly determine the optimal sensor placement to save energy and minimize the number of sensors for a building, which is a de-trial task for the cloud platform due to the limited number of sensors available and massive candidate locations for each sensor. In this paper, we propose a Fast Sensor Placement Location Optimization approach (FSPLO) to solve the BIM problem in cloud-aided smart buildings. In particular, we quickly filter out the repeated candidate locations of sensors in FSPLO using Locality Sensitive Hashing (LSH) techniques to maintain only a small number of optimized locations for deploying sensors around buildings. In this way, we can significantly reduce the number of sensors used for health and green buildings. Finally, a set of simulation experiments demonstrates the excellent performance of our proposed FSPLO method."
10.1186/s13677-023-00402-0,A fine-grained task scheduling mechanism for digital economy services based on intelligent edge and cloud computing,2023-03-04,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Digital economy is regarded countries as an inevitable choice to promote economic growth and provides new opportunities and new paths for the high-quality development of economy. For the Chinese market, the strong base behind cloud computing is the unstoppable development trend of the digital economy. In digital economy, the cloud as infrastructure becomes the base of the pyramid to build the digital economy. To relieve the pressure on the servers of the digital economy and develop a reasonable scheduling scheme, this paper proposes a fine-grained task scheduling method for cloud and edge computing based on a hybrid ant colony optimization algorithm. The edge computing task scheduling problem is described, and assumptions are set to simplify the difficulty of a scheduling solution. The multi-objective function is solved by using a hybrid ant colony optimization algorithm which solves computational problems by finding the optimal solution with the help of graphs. Ant colony optimization algorithm is easy to use and effective in scheduling problems. The proposed scheduling model includes an end-device layer and an edge layer. A terminal device layer consists of devices used by the clients that may generate computationally intensive tasks and are sometime uncapable to complete the tasks. The proposed scheduling policy migrates these tasks to a suitable place where they can be completed while meeting the latency requirements. The CPUs of the idle users on the end-device layer are used for other CPU-overloaded terminals. The simulation results, in terms of energy consumptions, and task scheduling delays, show that the task scheduling performance is better under the application of this method and the obtained scheduling scheme is more reasonable."
10.1186/s13677-023-00399-6,A big data study of language use and impact in radio broadcasting in China,2023-03-03,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Broadcasting more educating and language-reviving contents are ways radio stations can help revitalize the use of the English language in the Hunan province of China. The challenges faced in communicating in English in Chinese radio stations are majorly caused by the lack of language professionals and linguists in the broadcast stations. The absence of these professionals is a major constraint to the development of the community. The broadcast media can help manage multilingualism through the introduction of new words which would give little or no room for lexicon dearth but would expand the language lexicon. Using the English language during broadcast reduces language dearth, and helps reach a much larger audience, even those not in China. Programmes anchored in English in places where the language is barely spoken enhances the vocabulary, comprehension and language vitality of the listeners. This study examined the impact of the English language used in radio broadcasting using a descriptive Big Data survey research design. The study’s population comprises of the inhabitants of the Hunan province in China, from which a sample of 50 broadcast staff and 150 regular inhabitants was drawn using a stratified random sampling technique. The instrument of data collection was a structured questionnaire with closed questions and a self-structured interview. The sample employed frequency distribution tables, percentages, and charts in the presentation and analysis of data. The results revealed that majority of the respondents in Hunan listened to radio broadcast indicating that the use of English language can have massive impact on the people. The study also found that majority of the respondents use their indigenous languages in their day-to-day activities as well as their schools with English being used majorly only in schools with only English-speaking students. The study recommends, amongst others, that the Broadcasting Corporation of China (BCC) review their policy on the allocated time of broadcast in English languages, and that more English language experts and linguists should be incorporated into the broadcast system."
10.1186/s13677-023-00404-y,A privacy protection approach in edge-computing based on maximized dnn partition strategy with energy saving,2023-03-03,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the development of deep neural network (DNN) techniques, applications of DNNs show state-of-art performance. In the cloud edge collaborative mode, edge devices upload the raw data, such as texts, images, and videos, to the cloud for processing. Then, the cloud returns prediction or classification results. Although edge devices take advantage of the powerful performance of DNN, there are also colossal privacy protection risks. DNN partition strategy can effectively solve the privacy problems by offload part of the DNN model to the edge, in which the encoded features are transmitted rather than original data. We explore the relationship between privacy and the intermedia result of the DNN. The more parts offloaded to the edge, the more abstract features we can have, indicating more conducive to privacy protection. We propose a privacy protection approach based on a maximum DNN partition strategy. Besides, a mix-precision quantization approach is adopted to reduce the energy use of edge devices. The experiments show that our method manages to increase at most 20% model privacy in various DNN architecture. Through the energy-aware mixed-precision quantization approach, the model’s energy consumption is reduced by at most 5x comparing to the typical edge-cloud solution."
10.1186/s13677-023-00406-w,OpenStackDP: a scalable network security framework for SDN-based OpenStack cloud infrastructure,2023-02-28,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Network Intrusion Detection Systems (NIDS) and firewalls are the de facto solutions in the modern cloud to detect cyberattacks and minimize potential hazards for tenant networks. Most of the existing firewalls, perimeter security, and middlebox solutions are built on static rules/signatures or simple rule matching, making them inflexible, susceptible to bugs, and difficult to introduce new services. This paper aims to improve network management in OpenStack Clouds by taking advantage of the combination of software-defined networking (SDN), Network Function Virtualization (NFV), and machine learning/artificial intelligence (ML/AI) and for making networks more predictable, reliable, and secure. Artificial intelligence is being used to monitor the behavior of the virtual machines and applications running in the OpenStack SDN cloud so that when any issues or degradations are noticed, the decision can be quickly made on how to handle that issue, being able to analyze data in motion, starting at the edge. The OpenStackDP framework comprises lightweight monitoring, anomaly-detecting intelligent sensors embedded in the data plane, a threat analytics engine based on ML/AI algorithms running inside switch hardware/network co-processor, and defensive actions deployed as virtual network functions (VNFs). This network data plane-based architecture makes high-speed threat detection and rapid response possible and enables a much higher degree of security. We have built the framework with advanced streaming analytics technologies, algorithms, and machine learning to draw knowledge from this data that is in motion before the malicious traffic goes to the tenant compute nodes or long-term data store. Cloud providers and users will benefit from improved Quality-of-Services (QoS) and faster recovery from cyber-attacks and compromised switches. The multi-phase collaborative anomaly detection scheme demonstrates an accuracy of 99.81%, average latencies of 0.27 ms, and response speed within 9 s. The simulations and analysis show that the OpenStackDP network analytics framework substantially secures and outperforms prior SDN-based OpenStack solutions for Cloud architectures."
10.1186/s13677-023-00405-x,Memory sharing for handling memory overload on physical machines in cloud data centers,2023-02-28,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Over-committing computing resources is a widely adopted strategy for increased cluster utilization in Infrastructure as a Service (IaaS) cloud data centers. A potential consequence of over-committing computing resources is memory overload of physical machines (PMs). Memory overload occurs if memory usage exceeds a defined alarm threshold, exposing running computation tasks at a risk of being terminated by the operating system. A prevailing measure to handle memory overload of a PM is live migration of virtual machines (VMs). However, this not only consumes network bandwidth, CPU, and other resources, but also compels a temporary unavailability of the VMs being migrated. To handle memory overload, we present a memory sharing system in this paper for PMs in cloud data centers. With memory sharing, a PM automatically borrows memory from a remote PM when necessary, and releases the borrowed memory when memory overload disappears. This is implemented through swapping inactive memory pages to remote memory resource. Experimental studies conducted on InfiniBand-networked PMs show that the memory sharing system is fully functional. The measured throughput and latency are around 929 Mbps and 1.3 $$\mu$$ μ s, respectively, on average for remote memory access. They are similar to those from accessing a local-volatile memory express solid-state drive, and thus are promising in real applications."
10.1186/s13677-023-00403-z,DU-Net-Cloud: a smart cloud-edge application with an attention mechanism and U-Net for remote sensing images and processing,2023-02-27,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","In recent ages, the use of deep learning approaches to extract ground object information from remote sensing high-resolution images has attracted extensive attention in many fields. Nevertheless, due to the high similarity of features between roads, prevailing deep learning semantic segmentation networks commonly demonstrate reduced continuity in road segmentation. Besides this, the role of advanced computing technologies including cloud and edge infrastructures has also become very important due to large number of images and their storage requirements. In order to better study the road details in images related to remote sensing, this paper suggests a road extraction technique which is basically founded on Dimensional U-Net (DU-Net) network. At the deepening level of the U-Net network, a parallel attention mechanism, known as ProCBAM, is added and implemented to the feature transmission step of the classical U-Net network. Moreover, we use and implement the edge-cloud architecture to develop and construct a unique remote sensing image service system that integrates several datacenters and their related edge infrastructure. In the proposed system, the edge network is primarily used for caching and distributing the processed remote sensing images, while the remote datacenter serves as the cloud platform and is responsible for the storage and processing of original remote sensing images. The results show that the proposed cloud enabled DU-Net model has achieved good performance in road segmentation. We observed that it can achieve improved road segmentation and resolve the issue of reduced continuity of road segmentation when compared with other state-of-the-art learning networks. Moreover, our empirical evaluations suggest that the proposed system not only distributes the workload of processing tasks across the edges but also achieves data efficiency among them, which enhances image processing efficiency and reduces data transmission costs."
10.1186/s13677-023-00401-1,Improved wild horse optimization with levy flight algorithm for effective task scheduling in cloud computing,2023-02-21,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud Computing, the efficiency of task scheduling is proportional to the effectiveness of users. The improved scheduling efficiency algorithm (also known as the improved Wild Horse Optimization, or IWHO) is proposed to address the problems of lengthy scheduling time, high-cost consumption, and high virtual machine load in cloud computing task scheduling. First, a cloud computing task scheduling and distribution model is built, with time, cost, and virtual machines as the primary factors. Second, a feasible plan for each whale individual corresponding to cloud computing task scheduling is to find the best whale individual, which is the best feasible plan; to better find the optimal individual, we use the inertial weight strategy for the Improved whale optimization algorithm to improve the local search ability and effectively prevent the algorithm from reaching premature convergence. To deliver services and access to shared resources, Cloud Computing (CC) employs a cloud service provider (CSP). In a CC context, task scheduling has a significant impact on resource utilization and overall system performance. It is a Nondeterministic Polynomial (NP)-hard problem that is solved using metaheuristic optimization techniques to improve the effectiveness of job scheduling in a CC environment. This incentive is used in this study to provide the Improved Wild Horse Optimization with Levy Flight Algorithm for Task Scheduling in cloud computing (IWHOLF-TSC) approach, which is an improved wild horse optimization with levy flight algorithm for cloud task scheduling. Task scheduling can be addressed in the cloud computing environment by utilizing some form of symmetry, which can achieve better resource optimization, such as load balancing and energy efficiency. The proposed IWHOLF-TSC technique constructs a multi-objective fitness function by reducing Makespan and maximizing resource utilization in the CC platform. The IWHOLF-TSC technique proposed combines the wild horse optimization (WHO) algorithm and the Levy flight theory (LF). The WHO algorithm is inspired by the social behaviours of wild horses. The IWHOLF-TSC approach's performance can be validated, and the results evaluated using a variety of methods. The simulation results revealed that the IWHOLF-TSC technique outperformed others in a variety of situations."
10.1186/s13677-023-00389-8,Model-based cloud service deployment optimisation method for minimisation of application service operational cost,2023-02-18,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Many currently existing cloud cost optimisation solutions are aimed at cloud infrastructure providers, and they often deal only with specific types of application services. Unlike infrastructure providers, the providers of cloud applications are often left without a suitable cost optimisation solution, especially concerning the wide range of different application types. This paper presents an approach that aims to provide an optimisation solution for the providers of applications hosted in the cloud environments, applicable at the early phase of a cloud application lifecycle and for a wide range of application services. The focus of this research is the development of the method for identifying optimised service deployment option in available cloud environments based on the model of the service and its context, intending to minimise the operational cost of the cloud service while fulfilling the requirements defined by the service level agreement. A cloud application context metamodel is proposed that includes parameters related to both the application service and the cloud infrastructure relevant for the cost and quality of service. By using the proposed optimisation method, knowledge is gained about the effects of the cloud application context parameters on the service cost and quality of service, which is then used to determine the optimal service deployment option. The service models are validated using cloud applications deployed in laboratory conditions, and the optimisation method is validated using the simulations based on the proposed cloud application context metamodel. The experimental results based on two cloud application services demonstrate the ability of the proposed approach to provide relevant information about the impact of cloud application context parameters on service cost and quality of service and use this information for reducing service operational cost while preserving the acceptable service quality level. The results indicate the applicability and relevance of the proposed approach for cloud applications in the early service lifecycle phase since application providers can gain valuable insights regarding service deployment decision without acquiring extensive datasets for the analysis."
10.1186/s13677-022-00382-7,A bidirectional DNN partition mechanism for efficient pipeline parallel training in cloud,2023-02-16,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Recently, deep neural networks (DNNs) have shown great promise in many fields while their parameter sizes are rapidly expanding. To break through the computation and memory limitation of a single machine, pipeline model parallelism is proposed for large-scale DNN training by fully utilizing the computation and storage power of the distributed cluster. Cloud data centers can also provide sufficient computing, storage and bandwidth resources. However, most existing approaches apply layer-wise partitioning, which is difficult to obtain an even model partition result because of the large computational overhead discrepancy between DNN layers, resulting in degraded efficiency. To tackle this issue, we propose “Bi-Partition”, a novel partitioning method based on bidirectional partitioning for forward propagation (FP) and backward propagation (BP), which improves the efficiency of the pipeline model parallelism system. By deliberated designing distinct cut positions for FP and BP of DNN training, workers in the pipeline get nearly equal computational loads, and the balanced pipeline fully utilizes the computing resources. Experiments on various DNN models and datasets validate the efficiency of our mechanism, e.g., the training efficiency achieving up to 1.9 $$\times$$ × faster than the state-of-the-art method PipeDream."
10.1186/s13677-022-00379-2,A new dynamic security defense system based on TCP_REPAIR and deep learning,2023-02-14,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Honeypot is an active defense mechanism, which attracts attackers to interact with virtual resources in the honeypot mainly by simulating real working scenarios and deploying decoy targets, so as to prevent real resources from being damaged and collect attackers’ attack processes and analyze potential system vulnerabilities to proactively respond to similar attacks. Because of the existing honeypot system has defects such as the inability to deploy specific honeypots to induce attacks based on complex attacks, the inability to select the best honeypot for dynamic response based on honeypot deployment and maintenance costs during attack interactions, and insufficient ability to identify variants of known attack methods. Although hybrid honeypots can solve some of these problems by deploying low-interaction honeypots and high-interaction honeypots, they cannot really be applied to real production scenarios because of their slow TCP connection switching speed and inability to efficiently identify encrypted malicious traffic. In this paper, we propose a new dynamic security defense system based on the combination of TCP_REPAIR-based dynamic honeypot selection architecture and a deep learning-based intelligent firewall. The system accurately distributes encrypted or non-encrypted attack traffic and its variants through the intelligent firewall. The normal traffic is sent to the actual system, and the marked malicious traffic dynamically selects honeypots to respond according to the attack process.The experimental result indicated that the system can select honeypots for targeted responses according to the actual network situation quickly and dynamically and covertly, effectively improving the utilization rate of honeypot clusters as well as the ability to decoy."
10.1186/s13677-023-00398-7,A microservice regression testing selection approach based on belief propagation,2023-02-13,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Regression testing is required to assure the quality of each iteration of microservice systems. Test case selection is one of main techniques to optimize regression testing. Existing techniques mainly involve artifacts acquisition, processing and maintenance, thus hard to apply in microservice regression testing since it is difficult to obtain and process required artifacts from multiple development teams, which is normal in cases of microservice systems. This paper proposes a novel approach, namely MRTS-BP, which takes API gateway logs instead of artifacts as inputs. By mining service dependencies from API gateway logs, MRTS-BP analyzes service change impacts based on a propagation calculation, and selects test cases affected by changes based on impact degree values. To evaluate the effectiveness of MRTS-BP, empirical studies based on four real deployed systems are presented. Retest-all strategy and a regression testing selection approach based on control flow graphs called RTS-CFG are compared with MRTS-BP. The results show that, MRTS-BP can significantly reduce both the number of test cases and overall time cost while maintaining the fault detection capability of selected test suite, and that MRTS-BP can save more time cost than RTS-CFG with the similar safety and precision."
10.1186/s13677-023-00400-2,Efficiency and optimization of government service resource allocation in a cloud computing environment,2023-02-10,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","According to the connotation and structure of government service resources, data of government service resources in L city from 2019 to 2021 are used to calculate the efficiency of government service resource allocation in each county and region in different periods, particularly by adding the government cloud platform and cloud computing resources to the government service resource data and applying the data envelopment analysis (DEA) method, which has practical significance for the development and innovation of government services. On this basis, patterns and evolutionary trends of government service resource allocation efficiency in each region during the study period are analyzed and discussed. Results are as follows. i ) Overall efficiency level in the allocation of government service resources in L city is not high, showing an increasing annual trend among the high and low staggering. ii ) Relative difference of allocation efficiency of government service resources is a common phenomenon of regional development, the existence and evolution of which are the direct or indirect influence and reflection of various aspects, such as economic strength and reform effort. iii ) Data analysis for the specific points indicates that increased input does not necessarily lead to increased efficiency, some indicators have insufficient input or redundant output. Therefore, optimization of the physical, human, and financial resource allocation methods; and the intelligent online processing of government services achieved by the adoption of government cloud platform and cloud computing resources are the current objective choices to realize maximum efficiency in the allocation of government service resources."
10.1186/s13677-023-00394-x,Privacy-preserving cloud-edge collaborative learning without trusted third-party coordinator,2023-02-10,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud-edge collaborative learning has received considerable attention recently, which is an emerging distributed machine learning (ML) architecture for improving the performance of model training among cloud center and edge nodes. However, existing cloud-edge collaborative learning schemes cannot efficiently train high-performance models on large-scale sparse samples, and have the potential risk of revealing the privacy of sensitive data. In this paper, adopting homomorphic encryption (HE) cryptographic technique, we present a privacy-preserving cloud-edge collaborative learning over vertically partitioned data, which allows cloud center and edge node to securely train a shared model without a third-party coordinator, and thus greatly reduces the system complexity. Furthermore, the proposed scheme adopts the batching technique and single instruction multiple data (SIMD) to achieve parallel processing. Finally, the evaluation results show that the proposed scheme improves the model performance and reduces the training time compared with the existing methods; the security analysis indicates that our scheme can guarantee the security in semi-honest model."
10.1186/s13677-023-00391-0,QoS prediction in intelligent edge computing based on feature learning,2023-02-04,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the development of 5G and 6G, more computing and network resources on edge nodes are deployed close to the terminal. Meanwhile, the number of smart devices and intelligent services has grown significantly, which makes it difficult for users to choose a suitable service. The rich contextual information plays an important role in the prediction of service quality. In this paper, we propose a quality of service(QoS) prediction approach based on feature learning, the contextual information represented as the explicit features and underlying relationship hidden in the implicit features are fully considered. Then, the multi-head self-attention mechanism is used in the interacting layer to determine which features should be combined to form meaningful high-order features interaction. We have implemented our proposed approach with experiments based on real-world datasets. Experimental results show that our approach achieved a better performance of service QoS prediction in an intelligent edge computing environment for future communication."
10.1186/s13677-023-00393-y,Correction: Can cloudlet coordination support cloud computing infrastructure?,2023-02-03,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems",
10.1186/s13677-022-00374-7,"Task scheduling in cloud environment: optimization, security prioritization and processor selection schemes",2023-01-27,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Cloud computing is an extremely important infrastructure used to perform tasks over processing units. Despite its numerous benefits, a cloud platform has several challenges preventing it from carrying out an efficient workflow submission. One of these is linked to task scheduling. An optimization problem related to this is the maximal determination of cloud computing scheduling criteria. Existing methods have been unable to find the quality of service (QoS) limits of users- like meeting the economic restrictions and reduction of the makespan. Of all these methods, the Heterogeneous Earliest Finish Time (HEFT) algorithm produces the maximum outcomes for scheduling tasks in a heterogeneous environment in a reduced time. Reviewed literature proves that HEFT is efficient in terms of execution time and quality of schedule. The HEFT algorithm makes use of average communication and computation costs as weights in the DAG. In some cases, however, the average cost of computation and selecting the first empty slot may not be enough for a good solution to be produced. In this paper, we propose different HEFT algorithm versions altered to produce improved results. In the first stage (rank generation), we execute several methodologies to calculate the ranks, and in the second stage, we alter how the empty slots are selected for the task scheduling. These alterations do not add any cost to the primary HEFT algorithm, and reduce the makespan of the virtual machines’ workflow submissions. Our findings suggest that the altered versions of the HEFT algorithm have a better performance than the basic HEFT algorithm regarding decreased schedule length of the workflow problems. "
10.1186/s13677-022-00356-9,Extremely boosted neural network for more accurate multi-stage Cyber attack prediction in cloud computing environment,2023-01-23,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","There is an increase in cyberattacks directed at the network behind firewalls. An all-inclusive approach is proposed in this assessment to deal with the problem of identifying new, complicated threats and the appropriate countermeasures. In particular, zero-day attacks and multi-step assaults, which are made up of a number of different phases, some malicious and others benign, illustrate this problem well. In this paper, we propose a highly Boosted Neural Network to detect the multi-stageattack scenario. This paper demonstrated the results of executing various machine learning algorithms and proposed an enormously boosted neural network. The accuracy level achieved in the prediction of multi-stage cyber attacks is 94.09% (Quest Model), 97.29% (Bayesian Network), and 99.09% (Neural Network). The evaluation results of the Multi-Step Cyber-Attack Dataset (MSCAD) show that the proposed Extremely Boosted Neural Network can predict the multi-stage cyber attack with 99.72% accuracy. Such accurate prediction plays a vital role in managing cyber attacks in real-time communication."
10.1186/s13677-023-00390-1,A hybrid attention and time series network for enterprise sales forecasting under digital management and edge computing,2023-01-22,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Enterprises have both new opportunities and new challenges as a result of the rapid advancements in information technology that have accompanied the age of economic globalization. With the growth of internet of Things devices, data sizes have significantly increased. Further, the traditional cloud platform has been enriched with edge computing so that the huge data can be processed where it is collected. Therefore, businesses must adapt to new size requirements and rising standards for technical content. Forecasting corporate sales has emerged as a hot topic in the field of digital management. To successfully direct the future production and existence of enterprises, time series forecasting is of utmost importance and value. This is because it makes use of already-existing data to get the best predicting result. This work proposes a combination of enterprise sales forecasting from the perspective of digital management and neural networks, and proposes a network HATT-CNN-BiLSTM model for enterprise sales forecasting. First, this work combines multi-scale CNN (MSCNN) with improved BiLSTM (IBiLSTM) model. The MSCNN is utilized to extract spatial features with different scale, and it is often impossible to effectively explore the rules of time series features, and the processing of time series data is the strength of the LSTM network. Moreover, the IBiLSTM model can explore time series features in both directions, and therefore more useful information can be obtained. The MSCNN-IBiLSTM model, which is composed of MSCNN and IBiLSTM, can take advantage of strengths and avoid weaknesses, and give full play to the roles of the two models in different fields. Second, this work proposes a hybrid attention mechanism that combines self-attention, channel attention, and spatial attention. It enhances features extracted by MSCNN-IBiLSTM through a hybrid attention to build HATT-MSCNN-IBiLSTM network, which can extract more discriminative features. Third, this work conducts comprehensive and systematic experiments on HATT- MSCNN-IBiLSTM to verify feasibility of the proposed method. The proposed model is implemented over an edge computing platform that increases the model training speed and improve the response time."
10.1186/s13677-022-00388-1,Predicting the total Unified Parkinson’s Disease Rating Scale (UPDRS) based on ML techniques and cloud-based update,2023-01-21,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Nowadays, smart health technologies are used in different life and environmental areas, such as smart life, healthcare, cognitive smart cities, and social systems. Intelligent, reliable, and ubiquitous healthcare systems are a part of the modern developing technology that should be more seriously considered. Data collection through different ways, such as the Internet of things (IoT)-assisted sensors, enables physicians to predict, prevent and treat diseases. Machine Learning (ML) algorithms may lead to higher accuracy in medical diagnosis/prognosis based on health data provided by the sensors to help physicians in tracking symptom significance and treatment steps. In this study, we applied four ML methods to the data on Parkinson’s disease to assess the methods’ performance and identify the essential features that may be used to predict the total Unified Parkinson’s disease Rating Scale (UPDRS). Since accessibility and high-performance decision-making are so vital for updating physicians and supporting IoT nodes (e.g., wearable sensors), all the data is stored, updated as rule-based, and protected in the cloud. Moreover, by assigning more computational equipment and memory in use, cloud computing makes it possible to reduce the time complexity of the training phase of ML algorithms in the cases we want to create a complete structure of cloud/edge architecture. In this situation, it is possible to investigate the approaches with varying iterations without concern for system configuration, temporal complexity, and real-time performance. Analyzing the coefficient of determination and Mean Square Error (MSE) reveals that the outcomes of the applied methods are mostly at an acceptable performance level. Moreover, the algorithm’s estimated weight indicates that Motor UPDRS is the most significant predictor of Total UPDRS."
10.1186/s13677-022-00360-z,Wavelet transforms based ARIMA-XGBoost hybrid method for layer actions response time prediction of cloud GIS services,2023-01-20,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Layer actions response time is a critical indicator of cloud geographical information services (cloud GIS Services), which is of great significance to resource allocation and schedule optimization. However, since cloud GIS services are highly dynamic, uncertain, and uncontrollable, the response time of layer actions is influenced by spatiotemporal intensity and concurrent access intensity, posing significant challenges in predicting layer action response time.To predict the response time of layer actions more accurately, we analyzed the data association of cloud GIS services. Furthermore, based on the characteristics of long-term stable trends and short-term random fluctuations in layer actions response time series, a wavelet transforms-based ARIMA-XGBoost hybrid method for cloud GIS services is proposed to improve the one-step and multi-step prediction results of layer actions response time.We generate a multivariate time series feature matrix using the historical value of the layer actions response time, the predicted value of the linear component, and the historical value of the non-linear component. There is no need to meet the traditional assumption that the linear and nonlinear components of the time series are additive, which minimizes the model’s time series requirements and enhances its flexibility. The experimental results demonstrate the superiority of our approach over previous models in the prediction of layer actions response time of cloud GIS services."
10.1186/s13677-022-00387-2,Deep learning approach to security enforcement in cloud workflow orchestration,2023-01-18,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Supporting security and data privacy in cloud workflows has attracted significant research attention. For example, private patients’ data managed by a workflow deployed on the cloud need to be protected, and communication of such data across multiple stakeholders should also be secured. In general, security threats in cloud environments have been studied extensively. Such threats include data breaches, data loss, denial of service, service rejection, and malicious insiders generated from issues such as multi-tenancy, loss of control over data and trust. Supporting the security of a cloud workflow deployed and executed over a dynamic environment, across different platforms, involving different stakeholders, and dynamic data is a difficult task and is the sole responsibility of cloud providers. Therefore, in this paper, we propose an architecture and a formal model for security enforcement in cloud workflow orchestration. The proposed architecture emphasizes monitoring cloud resources, workflow tasks, and the data to detect and predict anomalies in cloud workflow orchestration using a multi-modal approach that combines deep learning, one class classification, and clustering. It also features an adaptation scheme to cope with anomalies and mitigate their effect on the workflow cloud performance. Our prediction model captures unsupervised static and dynamic features as well as reduces the data dimensionality, which leads to better characterization of various cloud workflow tasks, and thus provides better prediction of potential attacks. We conduct a set of experiments to evaluate the proposed anomaly detection, prediction, and adaptation schemes using a real COVID-19 dataset of patient health records. The results of the training and prediction experiments show high anomaly prediction accuracy in terms of precision, recall, and F1 scores. Other experimental results maintained a high execution performance of the cloud workflow after applying adaptation strategy to respond to some detected anomalies. The experiments demonstrate how the proposed architecture prevents unnecessary wastage of resources due to anomaly detection and prediction."
10.1186/s13677-022-00370-x,Cost optimization in cloud environment based on task deadline,2023-01-17,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The popularity of cloud and fog services has raised the number of users exponentially. Main advantage of Cloud/fog infrastructure and services are crucial specially for commercial users from diverse areas. The variety of service requests with different deadlines makes the task of a service broker challenging. The fog and cloud users always lookfor a suitable compromise between cost and quality of service in terms of response time therefore, the cost optimization is vital for the cloud/fog service providers to capture the market. In this paper an algorithm, Cost Optimization in the cloud/fog environment based on Task Deadline (COTD) is proposed that optimizes cost without compromising the response time. In this algorithm the task deadline is considered as a constraint and an appropriate data center for task processing is selected. The proposed algorithm is suitable for runtime decision making due to its low complexity. The proposed algorithm is evluated using a well-known simulation tool Cloud Analyst. Our comprehensive testbed simulations show that COTD outperforms the existing schemes, Service Proximity Based Routing and Performance-Optimized Routing. The proposed algorithm successfully minimizes the cost by 35% on average while maintaining the response time."
10.1186/s13677-023-00392-z,An enhanced ordinal optimization with lower scheduling overhead based novel approach for task scheduling in cloud computing environment,2023-01-17,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Efficient utilization of available computing resources in Cloud computing is one of the most challenging problems for cloud providers. This requires the design of an efficient and optimal task-scheduling strategy that can play a vital role in the functioning and overall performance of the cloud computing system. Optimal Schedules are specifically needed for scheduling virtual machines in fluctuating & unpredictable dynamic cloud scenario. Although there exist numerous approaches for enhancing task scheduling in the cloud environment, it is still an open issue. The paper focuses on an improved & enhanced ordinal optimization technique to reduce the large search space for optimal scheduling in the minimum time to achieve the goal of minimum makespan. To meet the current requirement of optimal schedule for minimum makespan, ordinal optimization that uses horse race conditions for selection rules is applied in an enhanced reiterative manner to achieve low overhead by smartly allocating the load to the most promising schedule. This proposed ordinal optimization technique and linear regression generate optimal schedules that help achieve minimum makespan. Furthermore, the proposed mathematical equation, derived using linear regression, predicts any future dynamic workload for a minimum makespan period target."
10.1186/s13677-022-00383-6,Robust and accurate performance anomaly detection and prediction for cloud applications: a novel ensemble learning-based framework,2023-01-14,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Effectively detecting run-time performance anomalies is crucial for clouds to identify abnormal performance behavior and forestall future incidents. To be used for real-world applications, an effective anomaly detection framework should meet three main challenging requirements: high accuracy for identifying anomalies, good robustness when application patterns change, and prediction ability for upcoming anomalies. Unfortunately, existing research about performance anomaly detection usually focuses on improving detection accuracy, while little research tackles the three challenges simultaneously. We conduct experiments for existing detection methods on multiple application monitoring data, and results show that existing detection methods usually focus on different features in data, which will lead to their diverse performance on different data patterns. Therefore, existing anomaly detection methods have difficulty improving detection accuracy and robustness and predicting anomalies. To address the three requirements, we propose an Ensemble Learning-Based Detection (ELBD) framework which integrates existing well-selected detection methods. The framework includes three classic linear ensemble methods (maximum, average, and weighted average) and a novel deep ensemble method. Our experiments show that the ELBD framework realizes better detection accuracy and robustness, where the deep ensemble method can achieve the most accurate and robust detection for cloud applications. In addition, it can predict anomalies in the next four minutes with an F1 score higher than 0.8. The paper also proposes a new indicator $$ARP\_score$$ A R P _ s c o r e to measure detection accuracy, robustness, and multi-step prediction ability. The $$ARP\_score$$ A R P _ s c o r e of the deep ensemble method is 5.1821, which is much higher than other detection methods."
10.1186/s13677-022-00367-6,Understanding the challenges and novel architectural models of multi-cloud native applications – a systematic literature review,2023-01-12,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","The evolution of Cloud Computing into a service utility, along with the pervasive adoption of the IoT paradigm, has promoted a significant growth in the need of computational and storage services. The traditional use of cloud services, focused on the consumption of one provider, is not valid anymore due to different shortcomings being the risk of vendor lock-in a critical. We are assisting to a change of paradigm, from the usage of a single cloud provider to the combination of multiple cloud service types, affecting the way in which applications are designed, developed, deployed and operated over such heterogeneous ecosystems. The result is an effective heterogeneity of architectures, methods, tools, and frameworks, copying with the multi-cloud application concept. The goal of this study is manifold. Firstly, it aims to characterize the multi-cloud concept from the application development perspective by reviewing existing definitions of multi-cloud native applications in the literature. Secondly, we set up the basis for the architectural characterization of these kind of applications. Finally, we highlight several open research issues drawn up from the analysis carried out. To achieve that, we have conducted a systematic literature review (SLR), where, a large set of primary studies published between 2011 and 2021 have been studied and classified. The in-depth analysis has revealed five main research trends for the improvement of the development and operation DevOps lifecycle of “multi-cloud native applications”. The paper finishes with directions for future work and research challenges to be addressed by the software community."
10.1186/s13677-022-00386-3,MBi-GRUMCONV: A novel Multi Bi-GRU and Multi CNN-Based deep learning model for social media sentiment analysis,2023-01-10,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Today, internet and social media is used by many people, both for communication and for expressing opinions about various topics in many domains of life. Various artificial intelligence technologies-based approaches on analysis of these opinions have emerged natural language processing in the name of different tasks. One of these tasks is Sentiment analysis, which is a popular method aiming the task of analyzing people’s opinions which provides a powerful tool in making decisions for people, companies, governments, and researchers. It is desired to investigate the effect of using multi-layered and different neural networks together on the performance of the model to be developed in the sentiment analysis task. In this study, a new, deep learning-based model was proposed for sentiment analysis on IMDB movie reviews dataset. This model performs sentiment classification on vectorized reviews using two methods of Word2Vec, namely, the Skip Gram and Continuous Bag of Words, in three different vector sizes (100, 200, 300), with the help of 6 Bidirectional Gated Recurrent Units and 2 Convolution layers (MBi-GRUMCONV). In the experiments conducted with the proposed model, the dataset was split into 80%-20% and 70%-30% training-test sets, and 10% of the training splits were used for validation purposes. Accuracy and F1 score criteria were used to evaluate the classification performance. The 95.34% accuracy of the proposed model has outperformed the studies in the literature. As a result of the experiments, it was found that Skip Gram has a better contribution to classification success."
10.1186/s13677-022-00358-7,Load balancing and service discovery using Docker Swarm for microservice based big data applications,2023-01-07,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Big Data applications require extensive resources and environments to store, process and analyze this colossal collection of data in a distributed manner. Containerization with cloud computing provides a pertinent remedy to accommodate big data requirements, however requires a precise and appropriate load-balancing mechanism. The load on servers increases exponentially with increased resource usage thus making load balancing an essential requirement. Moreover, the adjustment of containers accurately and rapidly according to load as per services is one of the crucial aspects in big data applications. This study provides a review relating to containerized environments like Docker for big data applications with load balancing. A novel scheduling mechanism of containers for big data applications established on Docker Swarm and Microservice architecture is proposed. The concept of Docker Swarm is utilized to effectively handle big data applications' workload and service discovery. Results shows that increasing workloads with respect to big data applications can be effectively managed by utilizing microservices in containerized environments and load balancing is efficiently achieved using Docker Swarm. The implementation is done using a case study deployed on a single server and then scaled to four instances. Applications developed using containerized microservices reduces average deployment time and continuous integration."
10.1186/s13677-022-00385-4,Intermediate data fault-tolerant method of cloud computing accounting service platform supporting cost-benefit analysis,2023-01-05,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","This study mainly aims at the intermediate data fault-tolerant method of cloud computing accounting service platform supporting cost-benefit analysis, which aims at providing cost-benefit analysis function for the platform and strengthening the fault-tolerant ability of the intermediate data of the platform. The invention discloses a method for constructing an enterprise cloud computing accounting service platform. Collect the internal and external accounting service system data of an enterprise by using an accounting service platform network environment provided by a cloud service provider. Transmit the data to a data processing and storage layer data warehouse for storage after data cleaning, extraction and processing. Then, call the collected data through the data processing and storage layer to analyze the transaction cost-benefit of the enterprise. The intermediate data fault-tolerant model is constructed. After being solved by the ant colony algorithm, the intermediate data generated in the process of cost-benefit analysis and other accounting services are fault-tolerant processed. Finally, the platform accounting service results are output to the interactive interface through the data output display layer. The highest data availability probability of the method proposed in this study is 0. 98, which indicates that the method has high data availability after fault-tolerant processing, and can effectively realize the interaction with users. The experimental analysis shows that the method proposed in this study can effectively analyze the transaction costs and benefits of enterprises. The probability of data availability after fault-tolerant processing is higher, and a load of reading and writing is lower."
10.1186/s13677-022-00384-5,Lightweight similarity checking for English literatures in mobile edge computing,2023-01-05,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","With the advent of information age, mobile devices have become one of the major convenient equipment that aids people’s daily office activities such as academic research, one of whose major tasks is to check the repetition rate or similarity among different English literatures. Traditional literature similarity checking solutions in cloud paradigm often call for intensive computational cost and long waiting time. To tackle this issue, in this paper, we modify the traditional literature similarity checking solution in cloud paradigm to make it suitable for the light-weight mobile edge environment. Furthermore, we put forward a lightweight similarity checking approach $$\mathrm {SC_{MEC}}$$ SC MEC for English literatures in mobile edge computing environment. To validate the advantages of $$\mathrm {SC_{MEC}}$$ SC MEC , we have designed massive experiments on a dataset. The reported experimental results show that $$\mathrm {SC_{MEC}}$$ SC MEC can deliver a satisfactory similarity checking result of literatures compared to other existing approaches."
10.1186/s13677-022-00363-w,Low-power multi-cloud deployment of large distributed service applications with response-time constraints,2023-01-03,"Computer Science,Computer Communication Networks,Special Purpose and Application-Based Systems,Information Systems Applications (incl.Internet),Computer Systems Organization and Communication Networks,Computer System Implementation,Software Engineering/Programming and Operating Systems","Distributed service applications make heavy use of clouds and multi-clouds, and must (i) meet service quality goals (e.g. response time) while (ii) satisfying cloud resource constraints and (iii) conserving power. Deployment algorithms must (iv) provide a solution meeting these requirements within a short time to be useful in practice. Very few existing deployment methods address the first three requirements, and those that do take too long to find a deployment. The Low-Power Multi-Cloud Application Deployment (LPD) algorithm fills this gap with a low-complexity heuristic combination of generalized graph partitioning between clouds, bin-packing within each cloud and queueing approximations to control the response time. LPD has no known competitor that quickly finds a solution that satisfies response time bounds. A host execution time approximation for contention is fundamental to achieving sufficient solution speed. LPD is intended for use by cloud managers who must simultaneously manage hosts and application deployments and plan capacity to offer services such as Serverless Computing. On 104 test scenarios deploying up to 200 processes with up to 240 replicas (for scaling), LPD always produced a feasible solution within 100 s (within 20 seconds in over three-quarters of cases). Compared to the Mixed Integer Program solution by CPLEX (which took a lot longer and was sometimes not found) LPD solutions gave power consumption equal to MIP in a third of cases and within 6% of MIP in 95% of cases. In 93% of all 104 cases the power consumption is within 20% of an (unachievable) lower bound. LPD is intended as a stand-alone heuristic to meet solution time restrictions, but could easily be adapted for use as a repair mechanism in a Genetic Algorithm."
